\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2020}
\usepackage{times}
\usepackage{tabularx}
\usepackage{latexsym}
\usepackage{microtype}
\renewcommand{\UrlFont}{\ttfamily\small}
\newcommand{\comment}[1]{\textcolor{red}{\bf \small [#1]}}
\newcommand{\spacemanidol}[1]{\textcolor{orange}{\bf \small [#1 --dc]}}
\aclfinalcopy 
\setlength\titlebox{5cm}
\newcommand\BibTeX{B\textsc{ib}\TeX}
\title{Final Thesis Proposal: Curriculum Learning methods In Training Methods For Language Representations}
\author{Daniel Campos}
\begin{document}
\maketitle
\section{Introduction}
Representing the underlying meaning of language through textual representations has been a longstanding research area in Natural Language Processing (NLP). Initially, researchers treated words as discrete integrated units where each word was assigned a numerical value. These representations provided a clean and simple way to encode meaning in a space-efficient way but were unable to capture underlying meanings of words or context surrounding them.  Seeking to encode more information about common word attributes, the NLP community explored vectorizing information about each word in manually crafted representations like WordNet \cite{Miller1992WordNetAL}. These representations encoded manually crafted linguistic properties of language which provided models some notion of language structure and similarity which systems could leverage downstream. Seeking to include context-based on textual corpus word meaning representations like Word2Vec \cite{Mikolov2013EfficientEO} and GloVe \cite{Pennington2014GloveGV} trained fixed vector representations using unsupervised machine learning methods. Using these methods, researchers built new methods to classical problems that took advantage of the closeness of words vector space. These fixed vector representations greatly improved the performance of many NLP systems but they struggled to adequately represent words with multiple meanings(e.g. fly or bat) or include sentence specific context. \\
Neural Language Models(NLMs) like ELMO \cite{Peters2018DeepCW} introduced the broader NLP community to contextual word representations and initiated a Cambrian Explosion research into novel language representations. Then, researchers leveraging the transformer's \cite{Vaswani2017AttentionIA} ability to capture long term dependencies, models like GPT \cite{Radford2018ImprovingLU} and BERT \cite{Devlin2019BERTPO} became the field standard for language and development in downstream tasks which provided new state of the art models in areas like Question answering \cite{Devlin2019BERTPO}, Paraphrase Detection \cite{Wang2018GLUEAM}, and information retrieval \cite{Nogueira2019MultiStageDR} to name a few. The success of NLMs drove research to countless new representation methods like AlBERT \cite{Lan2019ALBERTAL}, RoBERTa \cite{Liu2019RoBERTaAR}, SPANBERT\cite{Joshi2019SpanBERTIP} and KBBERT \cite{Peters2019KnowledgeEC}, what these representations learn \cite{Liu2019LinguisticKA}, \cite{Clark2019WhatDB} and how to evaluate them \cite{Talmor2019oLMpicsO}, \cite{Wang2018GLUEAM}. \\
As NLMs have become a field staple research into improving NLMs has exploded in popularity and has brought increased performance, size, training time and complexity. Top research labs invested millions of dollars in training models like MEGATRON \cite{Shoeybi2019MegatronLMTM}, XL-NET \cite{Yang2019XLNetGA}, Turing-NLG \cite{Rosset2020TNLG} which require 500+ top of the line GPU/TPUs and take days to train. While increasing model size, dataset size and training time has continued to improve models by most benchmark it leads to a natural question which will be the focus of this thesis: can leveraging approaches in training data selection \cite{Bengio2009CurriculumL} \cite{Karras2017ProgressiveGO} \cite{Tran2020SubsetSF} \cite{Chatterjee2017ProgressiveLF},  \cite{Settles2009ActiveLL}\cite{Berger1996AME} improve NLMs accuracy and training efficiency? Research the efficiency of NLM creation is now more relevant than ever as the current method of scaling architecture is expensive in terms of CO2 emissions, electrical usage, and computer cost  \cite{Strubell2019EnergyAP}. To best study, the effect of various regimes this research will focus on the effect training routines have on ELMO's performance on well-established benchmarks.
\section{Methods}
While there does not seem to exist current research on applying anything but random sampling to neural language modeling approaches have been well explored in areas like Generative Adversarial Networks, Machine Translation, and computer vision to varying degrees of success. In GANs training regimes focused on target output image size growth \cite{Karras2017ProgressiveGO} have proven very effective while in machine translation methods around sentence complexity have shown to be the best optimization point. In Machine Translation researchers have used curriculum methods to avoid hyperparameter tuning \cite{Kano2017StructuredBasedCL}, improve training methodology \cite{Guo2019FineTuningBC}, and modifying mini-batch sampling \cite{Kocmi2017CurriculumLA} to varying degrees of effectiveness. As language
modeling shares a lot with machine translations it will be the focus of literature review but I will still explore on broad methodologies.
Broadly speaking, since this research can be slow and computationally expensive my experiments will focus on fixing as many variables as possible to best measure the effect of the various curricula on NLMS
\subsection{Models}
The modeling required for this experiment breaks into three main areas: dataset creation/sub-sampling, NLM training, and Downstream task training and evaluation. \\
The dataset sampling methodology will be created using spacy.io and focused on broad, generalizable methods to produce the sub-sample datasets associated with the many training regimes. This methodology needs only to be reproducible since its curriculum creation needs only to happen once and is not bound to any model training. \\ For modeling, I will be using the open-source code released with the original ELMO paper \footnote{https://github.com/allenai/bilm-tf}. To get a broad enough sample I will implement character and token level models with 1, 2 and 4 stacked-layer representations. Since PyTorch does not offer our of the box implementation for curriculum learning I will have to create methods for training regimes. I estimate this will be the bulk of my time along with hyperparameter tuning and exploration in training Learning Rates. \\
The final and most established portion of experimentation is the evaluation portion. This is described more in-depth in subsequent sections but mostly will leverage existing open-source libraries.
\subsection{Dataset}
The majority of this research will be focused on fixing as many variables as possible to ensure we only capture the variations in model quality due to the training regime. To ensure this is captured in our training data all training and some evaluation will be done on the Wiki-103 dataset \cite{Merity2016PointerSM}. Introduced in 2016 the WikiText-103 is a diverse dataset that has become a benchmark for NLM evaluation and training. The dataset is a collection of over 100 million tokens coming from verified good and featured articles on Wikipedia. The dataset features 28,595 articles and has a vocabulary size of 267,735 and has become a central dataset in the field because of its size, the quality of its language and ease of use. This dataset will be used to create various training regimes and sim lair to other NLMS I will measure variations in perplexity on the Test portion of the dataset as a proxy for pure model quality. 
\subsection{Training Methodologies}
Broadly speaking my training methodologies will focus on a few sampling methodologies: perplexity, linguistically driven notion of complexity, methods derived from other applications, sample length, and random sample. Since all of my methods are derived on sentence-level complexity learned models may gain an increased understanding of sentence-level representations without an ability to represent longer contexts. As of current, I do not see a way around this dependency as all curriculum learning methods rely on some notion of granularity.\\ The first method, perplexity is likely the simplest to implement is derived from the work of Shannon et Al. \cite{Shannon1951PredictionAE} focused on the perplexity of each sentence in English. Broadly speaking, this method will produce N-sub samples of the broader dataset ordered by perplexity. \\
In my second method, linguistically driver complexity, I will build sub-samples based on the depth of parse trees. I choose this methodology because it provides a method of separating sentences by varying degrees of dependency complexity. This method should be simplest to introduce as it will rely on using a standard parser on the target corpus.\\
The third method derived from other existing literature is mostly a placeholder in case I come across a relevant way of data subsampling during my literature. \\
The fourth and final methods of sampling serve mostly as baselines to provide a good basis to understand the lift my heuristic methods can give. 
\subsection{Evaluation}
Following the trend in optimizing research cycles most evaluation will focus on the most commonly accepted benchmarks of NLM quality: Model perplexity and effect on downstream task learning. As previously mentioned, to evaluate NLM quality we will measure model perplexity on the test portion of the WikiText-103 dataset. The evaluation will focus on what training regime achieves the lowest perplexity but also on the speed to which each training regime converges on their minima. The other task that will be used for evaluation will be evaluating the various NLM performances when used to train a downstream task. To reproduce the original paper and benchmark broadly with other language models I will use the GLUE \cite{Wang2018GLUEAM} benchmark. It is worth noting that the explicit purpose is not to find the best representation of ELMO but explore the effects various training regimes can have on training time and model performance which can be used to further improve performance on many of the bigger models. 
\section{Possible Results}
Based on the effect seen in other uses of active/curriculum learning I believe the explained experiments will capture 2 broad effects: training speed and model quality. While I am unsure of how effective training regimes will be on the model quality I firmly believe that they should at least allow for much quicker training. Based on the broader literature, by training with a curriculum, models are less susceptible to both overfitting and getting stuck in local minima.
\section{Timeline}
My main goal is to have explored broadly how training regimes affect neural language models along with gaining a deep understanding in NLMs, how to evaluate their quality, and how methods applied in other fields can be leveraged for NLP. In the table below is my detailed plan for the timeline.
\clearpage
\begin{tabularx}{1.01\textwidth} { 
  | >{\raggedright\arraybackslash}X 
  | >{\centering\arraybackslash}X 
  | >{\centering\arraybackslash}X  | }
 \hline
 \textbf{Task} & \textbf{Description} & \textbf{Due Date}  \\
 \hline
 Literature Review  &  Broad overview of Language Representations, Transformers, Neural Language Models, Curriculum learning and active learning
 & {first draft 3/15 \newline
 tentative final draft 3/30} \\
 \hline
 NLM training pipeline & Basic implementations of ELMO in its various configurations training on WikiText-103. Functional evaluation on perplexity and downstream performance on SQUAD. Goal is to approximate the original paper performance & 04/10 \\ \hline
 Curriculum Creation Pipeline & Dependable and functional implementation of a pipeline that can create various curriculum's given an input corpus. Targeting 5 variations in each of the broad methodologies(perplexity, length, linguistics based on parse tree, method based on Machine Translation work). & 04/30 \\
 Training of NLMs & Broad training of NLMs with their respective curriculum's along with evaluation metrics. & 05/30\\ \hline
 1st Complete Draft & First complete Draft of thesis including all sections & 06/15 \\ \hline
 2nd Complete Draft & Second draft with potential additional experiments & 07/15 \\ \hline 
 Final Draft & Final Draft for review with broader department & 08/15
\end{tabularx}
\clearpage
\nocite{*}
\bibliographystyle{plain}
\bibliography{uwthesis}
\end{document}