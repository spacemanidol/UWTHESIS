@inproceedings{Pennington2014GloveGV,
  title={Glove: Global Vectors for Word Representation},
  author={Jeffrey Pennington and Richard Socher and Christopher D. Manning},
  booktitle={EMNLP},
  year={2014}
}
@inproceedings{Peters2019KnowledgeEC,
  title={Knowledge Enhanced Contextual Word Representations},
  author={Matthew E. Peters and Mark Neumann and IV RobertLLogan and Roy Schwartz and Vidur Joshi and Sameer Singh and Noah A. Smith},
  booktitle={EMNLP/IJCNLP},
  year={2019}
}
@inproceedings{Strubell2019EnergyAP,
  title={Energy and Policy Considerations for Deep Learning in NLP},
  author={Emma Strubell and Ananya Ganesh and Andrew McCallum},
  booktitle={ACL},
  year={2019}
}
@inproceedings{Devlin2019BERTPO,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  booktitle={NAACL-HLT},
  year={2019}
}
@inproceedings{Shannon1951PredictionAE,
  title={Prediction and entropy of printed English},
  author={Claude E. Shannon},
  year={1951}
}
@article{Talmor2019oLMpicsO,
  title={oLMpics - On what Language Model Pre-training Captures},
  author={Alon Talmor and Yanai Elazar and Yoav Goldberg and Jonathan Berant},
  journal={ArXiv},
  year={2019},
  volume={abs/1912.13283}
}
@inproceedings{Settles2009ActiveLL,
  title={Active Learning Literature Survey},
  author={Burr Settles},
  year={2009}
}
@article{Merity2016PointerSM,
  title={Pointer Sentinel Mixture Models},
  author={Stephen Merity and Caiming Xiong and James Bradbury and Richard Socher},
  journal={ArXiv},
  year={2016},
  volume={abs/1609.07843}
}
@article{Shoeybi2019MegatronLMTM,
  title={Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism},
  author={Mohammad Shoeybi and Mostofa Ali Patwary and Raul Puri and Patrick LeGresley and Jared Casper and Bryan Catanzaro},
  journal={ArXiv},
  year={2019},
  volume={abs/1909.08053}
}
@inproceedings{Kocmi2017CurriculumLA,
  title={Curriculum Learning and Minibatch Bucketing in Neural Machine Translation},
  author={Tom Kocmi and Ondrej Bojar},
  booktitle={RANLP},
  year={2017}
}
@article{Guo2019FineTuningBC,
  title={Fine-Tuning by Curriculum Learning for Non-Autoregressive Neural Machine Translation},
  author={Junliang Guo and Xu Tan and Linli Xu and Tao Qin and Enhong Chen and Tie-Yan Liu},
  journal={ArXiv},
  year={2019},
  volume={abs/1911.08717}
}
@inproceedings{Kano2017StructuredBasedCL,
  title={Structured-Based Curriculum Learning for End-to-End English-Japanese Speech Translation},
  author={Takatomo Kano and Sakriani Sakti and Satoshi Nakamura},
  booktitle={INTERSPEECH},
  year={2017}
}
@article{Karras2017ProgressiveGO,
  title={Progressive Growing of GANs for Improved Quality, Stability, and Variation},
  author={Tero Karras and Timo Aila and Samuli Laine and Jaakko Lehtinen},
  journal={ArXiv},
  year={2017},
  volume={abs/1710.10196}
}
@inproceedings{Tran2020SubsetSF,
  title={Subset Sampling For Progressive Neural Network Learning},
  author={Dat Thanh Tran and Moncef Gabbouj and Alexandros Iosifidis},
  year={2020}
}
@article{Berger1996AME,
  title={A Maximum Entropy Approach to Natural Language Processing},
  author={Adam L. Berger and Stephen Della Pietra and Vincent J. Della Pietra},
  journal={Computational Linguistics},
  year={1996},
  volume={22},
  pages={39-71}
}
@article{Chatterjee2017ProgressiveLF,
  title={Progressive Learning for Systematic Design of Large Neural Networks},
  author={Saikat Chatterjee and Alireza M. Javid and Mostafa Sadeghi and Partha P. Mitra and Mikael Skoglund},
  journal={ArXiv},
  year={2017},
  volume={abs/1710.08177}
}
@inproceedings{Bengio2009CurriculumL,
  title={Curriculum learning},
  author={Yoshua Bengio and J{\'e}r{\^o}me Louradour and Ronan Collobert and Jason Weston},
  booktitle={ICML '09},
  year={2009}
}
@inproceedings{Yang2019XLNetGA,
  title={XLNet: Generalized Autoregressive Pretraining for Language Understanding},
  author={Zhilin Yang and Zihang Dai and Yiming Yang and Jaime G. Carbonell and Ruslan Salakhutdinov and Quoc V. Le},
  booktitle={NeurIPS},
  year={2019}
}
@article{Liu2019RoBERTaAR,
  title={RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
  journal={ArXiv},
  year={2019},
  volume={abs/1907.11692}
}
@inproceedings{Radford2018ImprovingLU,
  title={Improving Language Understanding by Generative Pre-Training},
  author={Alec Radford},
  year={2018}
}
@article{Joshi2019SpanBERTIP,
  title={SpanBERT: Improving Pre-training by Representing and Predicting Spans},
  author={Mandar Joshi and Danqi Chen and Yinhan Liu and Daniel S. Weld and Luke Zettlemoyer and Omer Levy},
  journal={ArXiv},
  year={2019},
  volume={abs/1907.10529}
}
@article{Nogueira2019MultiStageDR,
  title={Multi-Stage Document Ranking with BERT},
  author={Rodrigo Nogueira and Wei Yang and Kyunghyun Cho and Jimmy Lin},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.14424}
}
@inproceedings{Wang2018GLUEAM,
  title={GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
  author={Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},
  booktitle={BlackboxNLP@EMNLP},
  year={2018}
}
@article{Clark2019WhatDB,
  title={What Does BERT Look At? An Analysis of BERT's Attention},
  author={Kevin Clark and Urvashi Khandelwal and Omer Levy and Christopher D. Manning},
  journal={ArXiv},
  year={2019},
  volume={abs/1906.04341}
}
@inproceedings{Liu2019LinguisticKA,
  title={Linguistic Knowledge and Transferability of Contextual Representations},
  author={Nelson F. Liu and Matt Gardner and Yonatan Belinkov and Matthew E. Peters and Noah A. Smith},
  booktitle={NAACL-HLT},
  year={2019}
}
@article{Lan2019ALBERTAL,
  title={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
  author={Zhen-Zhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},
  journal={ArXiv},
  year={2019},
  volume={abs/1909.11942}
}
@article{Smith2019ContextualWR,
  title={Contextual Word Representations: A Contextual Introduction},
  author={Noah A. Smith},
  journal={ArXiv},
  year={2019},
  volume={abs/1902.06006}
}
@article{Miller1992WordNetAL,
  title={WordNet: a lexical database for English},
  author={George A. Miller},
  journal={Commun. ACM},
  year={1992},
  volume={38},
  pages={39-41}
}
@article{Athiwaratkun2017MultimodalWD,
  title={Multimodal Word Distributions},
  author={Ben Athiwaratkun and Andrew Gordon Wilson},
  journal={ArXiv},
  year={2017},
  volume={abs/1704.08424}
}
@inproceedings{You2019LargeBO,
  title={Large Batch Optimization for Deep Learning: Training BERT in 76 minutes.},
  author={Yang You and Jing Li and Sashank J. Reddi and Jonathan Hseu and Sanjiv Kumar and Srinadh Bhojanapalli and Xiaodan Song and James Demmel and Kurt Keutzer and Cho-Jui Hsieh},
  year={2019}
}
@article{Mikolov2013EfficientEO,
  title={Efficient Estimation of Word Representations in Vector Space},
  author={Tomas Mikolov and Kai Chen and Gregory S. Corrado and Jeffrey Dean},
  journal={CoRR},
  year={2013},
  volume={abs/1301.3781}
}
@article{Weizenbaum1966ELIZAA,
  title={ELIZA â€” a computer program for the study of natural language communication between man and machine},
  author={Joseph Weizenbaum},
  journal={Commun. ACM},
  year={1966},
  volume={26},
  pages={23-28}
}
@inproceedings{opennmt,
  author    = {Guillaume Klein and
               Yoon Kim and
               Yuntian Deng and
               Jean Senellart and
               Alexander M. Rush},
  title     = {OpenNMT: Open-Source Toolkit for Neural Machine Translation},
  booktitle = {Proc. ACL},
  year      = {2017},
  url       = {https://doi.org/10.18653/v1/P17-4012},
  doi       = {10.18653/v1/P17-4012}
}
@misc{wang2019jiant,
    author = {Alex Wang and Ian F. Tenney and Yada Pruksachatkun and Katherin Yu and Jan Hula and Patrick Xia and Raghu Pappagari and Shuning Jin and R. Thomas McCoy and Roma Patel and Yinghui Huang and Jason Phang and Edouard Grave and Haokun Liu and Najoung Kim and Phu Mon Htut and Thibault F\'evry and Berlin Chen and Nikita Nangia and Anhad Mohananey and Katharina Kann and Shikha Bordia and Nicolas Patry and David Benton and Ellie Pavlick and Samuel R. Bowman},
    title = {\texttt{jiant} 1.2: A software toolkit for research on general-purpose text understanding models},
    howpublished = {\url{http://jiant.info/}},
    year = {2019}
}
@misc{Rosset2020TNLG,
    author = {Corby Rosset},
    title = {Turing-NLG: A 17-billion-parameter language model by Microsoft},
    howpublished = {\url{https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/}},
    year = {2020}
}
@article{Bahdanau2014NeuralMT,
  title={Neural Machine Translation by Jointly Learning to Align and Translate},
  author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
  journal={CoRR},
  year={2014},
  volume={abs/1409.0473}
}
@inproceedings{Rajpurkar2016SQuAD10,
  title={SQuAD: 100, 000+ Questions for Machine Comprehension of Text},
  author={Pranav Rajpurkar and Jian Zhang and Konstantin Lopyrev and Percy Liang},
  booktitle={EMNLP},
  year={2016}
}
@inproceedings{Vaswani2017AttentionIA,
  title={Attention is All you Need},
  author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  booktitle={NIPS},
  year={2017}
}
@inproceedings{Luong2015EffectiveAT,
  title={Effective Approaches to Attention-based Neural Machine Translation},
  author={Thang Luong and Hieu Pham and Christopher D. Manning},
  booktitle={EMNLP},
  year={2015}
}
@inproceedings{McCallum2000MaximumEM,
  title={Maximum Entropy Markov Models for Information Extraction and Segmentation},
  author={Andrew McCallum and Dayne Freitag and Fernando C Pereira},
  booktitle={ICML},
  year={2000}
}
@article{Peters2018DeepCW,
  title={Deep Contextualized Word Representations},
  author={Matthew E. Peters and Mark Neumann and Mohit Iyyer and Matt Gardner and Christopher Clark and Kenton Lee and Luke Zettlemoyer},
  journal={ArXiv},
  year={2018},
  volume={abs/1802.05365}
}