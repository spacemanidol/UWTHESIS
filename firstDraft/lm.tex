\section{Language Modeling}

\subsection{Effects of Language Models}

\iffalse
Energy and Policy Considerations for Deep Learning in NLP
-Wheras a decade ago most NLP models could be trained and developed on
a commodity laptop or server, many now require
multiple instances of specialized hardware such as
GPUs or TPUs, therefore limiting access to these
highly accurate models on the basis of finances
-We train all models on
a single NVIDIA Titan X GPU, with the exception of ELMo which was trained on 3 NVIDIA
GTX 1080 Ti GPUs. While training, we repeatedly query the NVIDIA System Management Interface2
to sample the GPU power consumption
and report the average over all samples.
-estimate single cost of training GPT-2 12,902-43008 and the architecture search and hyperprarameter tuning on some of these models can be $942,973–$3,201,722
-Study cost of development: Linguistically informed self-attention. 123 hyperparam grid searches, 4789a jobs. Summ of ~10000 gpu hours which is over 10k in just eletrical
- Researchers should prioritize computationally
efficient hardware and algorithms


HULK: An Energy Efficiency Benchmark Platform for Responsible Natural Language Processing
We introduce
HULK, a multi-task energy efficiency benchmarking platform for responsible natural language processing. With HULK, we compare
pretrained models’ energy efficiency from the
perspectives of time and cost. Baseline benchmarking results are provided for further analysis. The fine-tuning efficiency of different
pretrained models can differ a lot among different tasks and fewer parameter number does
not necessarily imply better efficiency. We
analyzed such phenomenon and demonstrate
the method of comparing the multi-task efficiency of pretrained models
\fi
\iffalse
Since training large LM takes days if not weeks researchers have produced novel training methods which allow complete training of a model like BERT in little over an hour \cite{You2019LargeBO}. The main method of schocastic optimization, LAMB, is able to increase batch sizes to over 30,000. 


We will be describing Contextual Word Representation building of Smith et al 2019 \cite{Smith2019ContextualWR} 



The goal of any language is to convey information. To measure the average amount of information conveyed in a message, we use a metric called “entropy", proposed by Claude Shannon . It should be noted that entropy in the context of language is related to, but not the same as, entropy in the context of thermodynamics.


An intuitive explanation of entropy for languages comes from Shannon himself in his landmark paper “Prediction and Entropy of Printed English" :

“The entropy is a statistical parameter which measures, in a certain sense, how much information is produced on the average for each letter of a text in the language. If the language is translated into binary digits (0 or 1) in the most efficient way, the entropy is the average number of binary digits required per letter of the original language."
Scaling laws for Neural Language Models(Kaplan 2020)
-Large models are more sample efficient reaching the same level of performance with fewer optimization steps and using fewer data points
-Performance depends more on scale than on model shape. In other words model performance is mostly based on model parameters, dataset size, and compute used for training.
-Each of the three factors have a power law relationship with performance as long as one of the factors isn't bottlenedcked. Within six orders of magnitude this relatiosnhip doesnt seem to change but must eventually flatten out
-Using the transformer arch they can predict model performance using power law before training
-Train on the Webtext2 and evaluate on the test set focused on decoder only transformer but also train lstm and universal transformer for comparison
-Vary modelsize from 768 to 1.5b params 
-Dataset size 22 million to 23 billion
-Comparing LSTM and transformer perform better on tokes appearing early in the context but cannot match performance on later tokens. Power law of language model holds
-They observe no deviation from straight power law trends at large for their factors. It is assumed that eventually this levels off because Language is not zero entropy.
-They estimate the best model possible would be loss of 1.7 nats/token with 104 pf days and N10^12 params and data 10^12 params with
Fine-Tuning pretrained language models 
-Authors experiment with the effects of finetuning of language models they vary the random seed on hundreds(2100) of BERT finetuning on GLUE. 
-Looking at what is effected by random seed: weight initialization and traning data order are found to have a huge variance out of sample performance. 
-On small datasets we observe fine tunning trials diverge part of the way through training so people should stop training less promising runs early.
Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers
-We first show that even though smaller Transformer models execute faster per iteration, wider and deeper models converge in significantly fewer steps. Moreover, this acceleration in convergence typically outpaces the additional computational overhead of using larger models
-Therefore, the most compute-efficient training strategy is to counter intuitively train extremely large models but stop after a small number of iterations.
-This constraint causes the (often implicit) goal of
model training to be maximizing compute efficiency: how
to achieve the highest model accuracy given a fixed amount
of hardware and training time
-In particular, there
is typically an implicit assumption that models must be
trained until convergence, which makes larger models appear less viable for limited compute budgets. We challenge
this assumption by demonstrating the opportunity to increase model size at the cost of convergence. Concretely,
we show that the fastest way to train Transformer models (Vaswani et al., 2017) is to substantially increase model
size but stop training very early
-We show that the optimal
model size is closely linked to the dataset size. In particular,
large models perform favorably in big data settings where
overfitting is a limited concern
-Experiment on Masked Language Modeling and Machine Translation. 
 For ROBERTA, we
vary the depth in {3, 6, 12, 18, 24}, and the hidden size in
{256, 512, 768, 1024, 1536}.
-Train a RobERTA style model with BERT corpus. Evaluate on GLUE
-WMT14 English→French dataset. We vary the model depth in
{2, 6, 8} and hidden size in {128, 256, 512, 1024, 2048}.
-Wider and deeper Transformer models are more sampleefficient than small models: they reach the same level of
performance using fewer gradient steps (Figures 3–5). Moreover, this increase in convergence outpaces the additional
computational overhead from increasing model size, even
though we need to use more steps of gradient accumulation.
Consequently, after adjusting for wall-clock time, the larger
models are faster to train than smaller models
- For the
masked language modeling task, the validation perplexity
weakly depends on the shape of the model. Instead, the
total number of model parameters is the key determiner of
the convergence rate. Thus, increasing either the width or
the depth is effective at accelerating model training. On the
other hand, the preferred way to scale models for MT is
to increase their width as wider models usually outperform
deep models in final performance
-This aligns with the findings of
McCandlish et al. (2018): training efficiency is maximized
when models are trained near some critical batch size
-LArger prunes better, quantizes better but can overfit more(but corpuses are not yet big enough for overfitting)
Exploring the limits of transfer learning with a unified text-to-text transformer
-show that training a 4x larger Transformer model is a good usage of 4x more compute
RoBERTa 
-Looking at bER they find it was undertrained and just by training the model longer they can beat the models that followed BERT. 
-They train the model longer, with bigger batches and more data, remove next sentence prediction objective, train on longer sequences, and dynamically change the masking pattern applied to training data. 
-No public common data so they produce 5 corpora totaling > 160GB uncompressed text. 
-Evaluate onf GLE and SQUAD and RACE
-Model is set to 12 layers, 768 hidden size, 12 attention head and 110m params. 
-Dynamic masking by duplicating training data 10 times and masking 10 different ways. 
-They explore NSP and find using individual sentence hurts performance on downstrea tasks. By removing NSP the model matches or slightly improves downstream task.
-They vary the training coprous (Training takes 1024 V100 GPUs)
-Performs better on GLUE on nearly everything(when comp[ared to XLNET( missed SST, MRPC, COLA, WNLI)])
-Without Data Augmentation they get the SOTA F1 and EM on SQUAD 2.0 `

XLNET
-Autoregressive factorizes likleyhood into a forward product P(X) = P(xt|x<t) but does not take into account the bidirectional nature of language.
-Auto encoder based pretraing does not perform an explicity desnity estimation of data but instead aims to reconstruct the original data from a corrupted input. 
-Use of symbols like [MASK] at train time but not finetunning produces a pretrain-finetune discrepancy.
-BERT also assumes that predicted tokens are independent of each other given unmasked tokens. 
-XLNET seeks to leverage AE and AR while avoiding limitations. It does so by modifying AR maximizing the expected log likelihood of a sequence wrt all permutations of factorization order. 
-Since its AR the model does not rely on data corropution. Basically novel pre training method.
-They use Transformer XL to deal with longer text sequences. 
-Does better on GLUE, Squad, RACE,
-Their contribution comes in their new proposed training objective. They propose a permutation language modeling objective. LEt Z be the set of all possible permutations of the length t index of sequence 1,2,...,T.  they sample a factorization order at time and decompose the liklehood according to the factorization order. 
-As expected this does better than BERT at everything(when trained on same corpus and same model size as BERT) and when scaled up on data  does better than roberta 


DistilBERT
-In this paper, we show that it is possible to reach similar performances on many downstream-tasks
using much smaller language models pre-trained with knowledge distillation, resulting in models
that are lighter and faster at inference time, while also requiring a smaller computational training
budget. Our general-purpose pre-trained models can be fine-tuned with good performances on several
downstream tasks, keeping the flexibility of larger models. We also show that our compressed models
are small enough to run on the edge, e.g. on mobile devices.
Using a triple loss, we show that a 40% smaller Transformer (Vaswani et al. [2017]) pre-trained
through distillation via the supervision of a bigger Transformer language model can achieve similar
performance on a variety of downstream tasks, while being 60% faster at inference time. Further
ablation studies indicate that all the components of the triple loss are important for best performances.
-is a compression technique in which
a compact model - the student - is trained to reproduce the behaviour of a larger model - the teacher -
or an ensemble of models
-e In the present work, the student - DistilBERT - has the same general architecture as BERT. The token-type embeddings and the pooler are removed while the number of layers
is reduced by a factor of 2. Most of the operations used in the Transformer architecture (linear
layer and layer normalisation) are highly optimized in modern linear algebra frameworks and our
investigations showed that variations on the last dimension of the tensor (hidden size dimension) have
a smaller impact on computation efficiency (for a fixed parameters budget) than variations on other
-: DistilBERT retains 97% of BERT performance and beats elmo by a fair margin. Evaluation done on SQAD and GLUE


Deep contextualized word representations Aka ELMO
First big implementation of a deep LM. 
They used stacked Bi direction LSTM. 
ELMO( Embeddings from Language Models). Unlike previous approaches  elmo represenations are contextual. 
-By stacking Bilstm higher level LSTM caputre context dependent aspect of word meaning while lower level model aspects of sytax(aka part of speech). 
-Both layers aere usefull for downstread training of tasks. 
-ELMOS improves SOTA and includes 20% error reduction. 
-Previous methods try to overcome the shorcoming of traditional word vectors by adding subword information, learning separate vectors for word senses, or many other types of amplifications.
-they move away from tradition word units by doing sobword units through character convolutions which allow incorporation of multi sense information downstream without specific training.
-Train on the billion word benchmark
-They do a forward and backward AR word model. They compute the probability of the sequence by modeling p(t1,t2,...tn) = product over k p(tk| t1,t2,t3,t4,t5) and the same backward. Both these get passed in the bilm and jointly optimize to the sum of backward and forward log prob
-They introduct the concept of using the represenatation of ELMO in downstram NLP tasks. "Add elmo to the supervicesd model we freeze weights, pass x  through elmos and use the elmo enhanced representation downstream.
-Final model is 2 stacked biLSTMs with 4096 units and 512 dimension projection along with a 2048 character ngram convulitonal filter. All three layers project down to a 512 represenation.
-After 10 epochs on 1B word corpus average perplexity on dataset is 39.7 backwards and forwards.  
-Evaluate on SQUAD, building on top of a Bi direction attention flow model \cite{ Seo et al 17}  but have replace LSTM for GRU. 
-On SQUAD they se a F1 improvement of 4.7% which is huge(beats all other sOTS by 1.4%) 11 member ensemble pushes f1 to 87.4. For context adding Voce to baseline adds 1.8%
-Also evaluate on SST-5, NER, Corref, SRL, SNLI all posing big gains independent of task.
-Suggestion is ELMO can probably be used  where word embeddings were used as input. They also find that ELMO at end can also produce some gains. They add to both sides of BidAF in squad and see modest(0.5) gains over input only
-To explore what information is caputured they look at word play with its top 4 nearest neighbors in GLOVE. Looking at sentence where play occours they find their model can understand the correct meaning of play in a context of play noun(acting play), play noun(a soccer play) where glove just gives play, football, playing , game
-Also explore using Word Sense Disambiguation. Top Layers have SOTA scores as hand crafted features.
-PoOS tagging. ELMO is competitive with hand crafted fine tunned bilstm .
-Adding elmo on downstream tasks also increases sample efficiency greatly. In SRL adding ELMO the model exceeds baseline maximum with a 98% decrease in number of updates to reach same perforamnce.
-Sample efficiency is even better with small datasets. Improvments for elmo are largest for smaller training sets and reduce the ammount of data neaded. For SRL elmo + model + 1% data == model + 10% data

Language Models are Unsupervised Multitask Learners aka gpt-2
-Other word represenations dont do well in a no training regime. Aka they need to fine tune. They show that GPT-2 can perform well downs tream tasks in a zero shot setting. 
-GPT 2 is a AR model trained unidicrectionally.
-They do their training on a custom dataset called webtext which contains all outbound links from Reddit which had at least 3 karma. 45 millions links which had the text automatically extracted. 
-They do some deduping and content clearing to get to 8m documents totaling 40gb of text. All wikipedia is removed since its a common source of data for other downstream tasks
-They train 4 models of varying size(117, 345, 762, 1542)m of params using stacked transformer models.  Vocab is 50,257, context size is 1024.
-Not disclosed how long they trained but it was a lot.  Ppl on Wikitext 103 is (37.5, 26.37, 22.05, 17.48 reltated to model size) 
-Test on Cildrens Book Test(close style prediction of 10 possible words), Lambada(Long distance dependencies)
-Winograd Schemechallange, Summarization, Translation, Question Answering, In 7 out of the 8 tasks GPT-2 outperforms all other LM in zero shot 

Improving language understanding by generative pre-training(GPT)
-Using stacked layers of transformers they employ a language modeling task. 
-AR pretraining on large text corpus of the Bookcorpus dataset which is about the same size as 1b word benchmark
-Fine tune on downstream tasks. (many of the now glue tasks)
-12 layer transformer model.  12 attention heads 768 dimensions. 
-On finetuned task they get SOTA for many of the tasks over existing non LM models
-They explore the impact of number of layers and find consitent gains that start to level off after 8. 
-We introduced a framework for achieving strong natural language understanding with a single
task-agnostic model through generative pre-training and discriminative fine-tuning. By pre-training
on a diverse corpus with long stretches of contiguous text our model acquires significant world
knowledge and ability to process long-range dependencies which are then successfully transferred to
solving discriminative tasks such as question answering, semantic similarity assessment, entailment
determination, and text classification, improving the state of the art on 9 of the 12 datasets we
study. Using unsupervised (pre-)training to boost performance on discriminative tasks has long
been an important goal of Machine Learning research. O


BERT
-Biderectional Encoding Representation for Transformers
-Unlike previous(like elmo) its bidrectional naturally 
-Two existing strategies for pre-trained language repre. Feature based and fine tune. ELMO is feautre based which use task specific arch that included the pretrained representation as features. Finetune like GPT introduces minimal task specific params and is trained by fine tunning all pretrained parameters. 
-Introduce the concept of Masked language modeling inspired by Cloze Task. Randomly masks some tokens from input and the objective is to predict the original vocabulary id. MLM forces the representation to use both forward and backward context. 
-Introduce Next Sentence Prediction task 
-Formalize pre-training and fine tuning(building on gpt) pretrain takes a modeling function and trains on a document corpus. Fine tune takes a task specific supervicsed label and updates the whole network to learn the representation. Unified arch across tasks. 
-L layers of transformer( 12 and 24), A attention head (12, 16) H hidden size(768, 1024) for Bert base and large.
-Use of wordpiece embeddings(breaks down a corpus into a specific size of word pieces seeking to keep most common words from being broken up runing becomes run and ing) of 30k tokens. 
-Introduce  special vocab tokens [CLS] being a special classification token which agggreates the sequence represenation for a classification task(similair to EOS).
-[SEP] toke to show separation between sentences which can also represent joint information about two sentences. 
-MLM is introduced so the word cannot see itself and predict itself(masking keeps the model from cheating). Very similair to Cloze task. They mask 15% of all wordpiece tokens in each sequence at random. When doing masking 80% of the time the mask token gets assigned, 10% a random token and 10% unchanged. The model tries to predict the original token with cross entropy loss
-NSP 50% of time real next sentence 50% random sentence. Seems to be beneficial for QnA and NLI
-Corpus is book corpus(800m words) + english wiki(2.5b words)
-For fine tunning they just plug task specific input and output into bert and fine tune all params end 2 end. Basically the CLS represenation is fed to a classification layer. 
-They evaluate on GLUE, SQUAd v1.1. On Glue they gain 4.5 and 7% over existing SOTA. On SQUAD they beat SOTA by 1.5%. On SQUAD 2.0 they see a 5.1 F1 over sota, on SWAG they see a 8.3% lead over GPT and 27.1 over SWAG ESIM+ELMO Baseline
-They see that without the NSP there is significatnt impact to QNLI, MNLI, SQUAD 1.1
-Without bidirectional training the model performs worse on everything. 
-Their small model beats other much larger models. 
-BERT takes as inpute the concatenation of two segments where segments usually consist of more than one natural sentence. The two segments are presented into a single inptu sequence with special tokens delimiting them [CLS] for start, [SEP] in middle and [EOS] in end. The model is pretrined on a large unlabled corpus and subsequentely fintuned usinging some downstream task data. 
-BERT uses the transformer architecture. with L layers, A self attention heads and H hidden dimensions. It only uses the encoder part. 
-BER trains towards Masked Langauge Modeling where a random sample of tokes in the input sequence is selected and replaced with a special token [MASK] and the model trains by predicting what the masked token is. BER uniformly slects 15% of input tokens for replacement. Of those selected 80% are masked, 10% arent changed, and 10% are replaced by a random vocabulary word. MAsking is done before training and thus is constant. 
-BERT trins also towards Next sentence prediction where 2 sentences are palce by eachother and the model trains towards predicting if sentence A is followed by sentence big. This is done to improve permormance between sentences and form a representation of common knowledge. 
-Models are trained for 1,000,000 batches with a batch size of 256 on the BookCORPUS, and English Wikipedia. 
-Uses character leven BPE of size 30k which is learned after preprocessing the inputs.

UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training
-. Given an input text with
masked tokens, we rely on conventional masks
to learn inter-relations between corrupted tokens
and context via autoencoding, and pseudo masks
to learn intra-relations between masked spans
via partially autoregressive modeling. With welldesigned position embeddings and self-attention
masks, the context encodings are reused to avoid
redundant computation
-Two types of pre training objectives: The first strand of work relies on autoencoding LMs(AKA BERT),  Autoregressive(GPT-, etc) .
-They propose joint training for LM for understanding(auto enconding) and generation(Auto regresive). They do this within one acchitecture with shared weights.  
-Aurto encoding is the same as BERT.
-Partially autoregressive on some portion of the text but with a specicifc pseudo marked token. Aka can be used for auto regresive. 
-Rather than replacing tokens  with masks as vanilla MLM they append a pseudo mask to the input sentence. 
-Their masking is differenent because 60^ of time they mask a tone 40% they mask an ngram. 
-they train both on the same corpus and input and the loss is the sum of AE and AR loss. 
-They test on SQUAD and GLUE and in both cases beat previous SOTA(ROBERTA) by a fair margin
-Experiments set the same size as BERT for consistenncy. Corpus for training they use the 160gb of text used by RoBERTa. Trained 20 days on 64 V100(holy fuck)
-Also evaluate on abstractive Summarixation CNN/DailyMail XSum. summarization task. Beats all other LMS.
Poor Man’s BERT: Smaller and Faster Transformer Models
-BERT, XLNet, and RoBERTa, are
now out of reach for researchers and practitioners without large-memory GPUs/TPUs. To
address this issue, we explore a number of
memory-light model reduction strategies that
do not require model pre-training from scratch
-The experimental results show that we are able
to prune BERT, RoBERTa and XLNet models
by up to 40\%, while maintaining up to 98\% of
their original performance. We also show that
our pruned models are on par with DistilBER
-They experiment with various methods of dropping layers of transformers. They evaluate what happens on GLUE.
-They find layer dropping can work well for all the models they tested even DistillBERT. They can remove one-third of its layers with an average loss of 0.75%
-They find the most effective methodology is to drop the top K layes from the models. This is in line with prior literature pointing to the top layers of models are focused on the underlying objective function. Thus Downsteam layers are likley more important for finetunning.
We can see
that by dropping half of the layers of the network,
the average performance drops between 1.81–3.28
points, the number of model parameters reduce by
40\% and the fine-tuning and inference time reduces
by 50\%
Our six-layer BERT (BERT-6) and XLNet (XLNet-6) showed competitive performance to
DistilBERT. They also showed comparable memory and speed performance as shown in Table 3.
This result is quite astonishing, given that our reduced models do not require any additional training, while building a DistilBERT model requires
training from scratch, which is a time consuming
and computation expensive process
-XLNET shows more robustness to dropping layers than bert

ELECTRA: PRE-Trainng Text Encoders as Discriminators rather than generators
-Authors explore training regime changes to the BERT methodlogy to make it more efficient and better. 
-Their model outperforms GPT on GLUE which was trained using 30x more compute along with a RoBERTa competitor for 1/4 the compute 
-Current langauge modeling focuses on Masked Langauge Modeling where ~15\% of tokens are masked and the model trains to predict the masked token. 
-Their alternative is replaced token detection where the model guesses which of the words in the setence has been replaced. This methodology is used to train transformer models to much success. The
-They train two neural networks, a generator G and discriminator D. Each one consits of an encoder that maps a sequence on input tokens into a contextualized vector represenatation. For a given position in an input the discriminator predicts if a given token is fake/replaced. 
-They explore various training strategies like only training the generator with a Masked Language model for N steps but it didnt produce good scores.  neither did having shared weight initialization. 
-Goal of this work was to develop a model that can train on a single gpu 
Compared to ELMO (71.2 on GLUE  for 96m params) they achieve 79.0 for their small model(14M params and 85.1 on the large(110M))

ALBERT
-Asks the question: is having better NLP models as easy as having larger models? Answer is no. If they train a BERT model that is even bigger(2048 hidden size, 1270M params) they find a ~20\% drop in RACE accuracy.
-ALBERT splits the vocabulary embedding matrix into two small matrix and cross layer param sharing they can grow the network without growing the parameter count. They use this to make ALBER have 18x less params  which trains 1.7x faster
-Introduce a new self supervised loss for Sentence order prediction which was created to address the ineffectiveness of the next sentence prediction loss in the original BERT. Since they can scale without exploding param caount they scale up albert to get new SOTA on GLUE, SQuad and RACE benchamks. 
-Changes include sharing parameters across layers of transformers(meaning they can add more layers without much issue), Sentence ordering traning, and factorization of the embedding parameters.
-ALBERT base is ~ 1/8 the size of BERT base
-Show that having dropout in Transformer based LM can hurt performance.
\fi