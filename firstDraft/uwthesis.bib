@article{Sun2019ERNIE2A,
  title={ERNIE 2.0: A Continual Pre-training Framework for Language Understanding},
  author={Yu Sun and Shuohuan Wang and Yukun Li and Shikun Feng and Hao Tian and Hua Wu and Haifeng Wang},
  journal={ArXiv},
  year={2019},
  volume={abs/1907.12412}
}
@inproceedings{McCann2017LearnedIT,
  title={Learned in Translation: Contextualized Word Vectors},
  author={Bryan McCann and James Bradbury and Caiming Xiong and Richard Socher},
  booktitle={NIPS},
  year={2017}
}
@inproceedings{Pennington2014GloveGV,
  title={Glove: Global Vectors for Word Representation},
  author={Jeffrey Pennington and Richard Socher and Christopher D. Manning},
  booktitle={EMNLP},
  year={2014}
}
@inproceedings{Peters2019KnowledgeEC,
  title={Knowledge Enhanced Contextual Word Representations},
  author={Matthew E. Peters and Mark Neumann and IV RobertLLogan and Roy Schwartz and Vidur Joshi and Sameer Singh and Noah A. Smith},
  booktitle={EMNLP/IJCNLP},
  year={2019}
}
@inproceedings{Strubell2019EnergyAP,
  title={Energy and Policy Considerations for Deep Learning in NLP},
  author={Emma Strubell and Ananya Ganesh and Andrew McCallum},
  booktitle={ACL},
  year={2019}
}
@inproceedings{Devlin2019BERTPO,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  booktitle={NAACL-HLT},
  year={2019}
}
@inproceedings{Shannon1951PredictionAE,
  title={Prediction and entropy of printed English},
  author={Claude E. Shannon},
  year={1951}
}
@article{Talmor2019oLMpicsO,
  title={oLMpics - On what Language Model Pre-training Captures},
  author={Alon Talmor and Yanai Elazar and Yoav Goldberg and Jonathan Berant},
  journal={ArXiv},
  year={2019},
  volume={abs/1912.13283}
}
@inproceedings{Settles2009ActiveLL,
  title={Active Learning Literature Survey},
  author={Burr Settles},
  year={2009}
}
@article{Merity2016PointerSM,
  title={Pointer Sentinel Mixture Models},
  author={Stephen Merity and Caiming Xiong and James Bradbury and Richard Socher},
  journal={ArXiv},
  year={2016},
  volume={abs/1609.07843}
}
@article{Shoeybi2019MegatronLMTM,
  title={Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism},
  author={Mohammad Shoeybi and Mostofa Ali Patwary and Raul Puri and Patrick LeGresley and Jared Casper and Bryan Catanzaro},
  journal={ArXiv},
  year={2019},
  volume={abs/1909.08053}
}
@inproceedings{Kocmi2017CurriculumLA,
  title={Curriculum Learning and Minibatch Bucketing in Neural Machine Translation},
  author={Tom Kocmi and Ondrej Bojar},
  booktitle={RANLP},
  year={2017}
}
@article{Sutton1998ReinforcementLA,
  title={Reinforcement Learning: An Introduction},
  author={Richard S. Sutton and Andrew G. Barto},
  journal={IEEE Transactions on Neural Networks},
  year={1998},
  volume={16},
  pages={285-286}
}
@article{Sharir2020TheCO,
  title={The Cost of Training NLP Models: A Concise Overview},
  author={Or Sharir and Barak Peleg and Yoav Shoham},
  journal={ArXiv},
  year={2020},
  volume={abs/2004.08900}
}
@article{Hochreiter1997LongSM,
  title={Long Short-Term Memory},
  author={Sepp Hochreiter and J{\"u}rgen Schmidhuber},
  journal={Neural Computation},
  year={1997},
  volume={9},
  pages={1735-1780}
}
@article{Platanios2019CompetencebasedCL,
  title={Competence-based Curriculum Learning for Neural Machine Translation},
  author={Emmanouil Antonios Platanios and Otilia Stretcu and Graham Neubig and Barnab{\'a}s P{\'o}czos and Tom Michael Mitchell},
  journal={ArXiv},
  year={2019},
  volume={abs/1903.09848}
}
@article{Chelba2014OneBW,
  title={One billion word benchmark for measuring progress in statistical language modeling},
  author={Ciprian Chelba and Tomas Mikolov and Michael Schuster and Qi Ge and Thorsten Brants and Phillipp Koehn and Tony Robinson},
  journal={ArXiv},
  year={2014},
  volume={abs/1312.3005}
}
@article{Kaplan2020ScalingLF,
  title={Scaling Laws for Neural Language Models},
  author={Jean Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
  journal={ArXiv},
  year={2020},
  volume={abs/2001.08361}
}
@inproceedings{Kiros2015SkipThoughtV,
  title={Skip-Thought Vectors},
  author={Ryan Kiros and Yukun Zhu and Ruslan Salakhutdinov and Richard S. Zemel and Raquel Urtasun and Antonio Torralba and Sanja Fidler},
  booktitle={NIPS},
  year={2015}
}
@article{Bojanowski2017EnrichingWV,
  title={Enriching Word Vectors with Subword Information},
  author={Piotr Bojanowski and Edouard Grave and Armand Joulin and Tomas Mikolov},
  journal={Transactions of the Association for Computational Linguistics},
  year={2017},
  volume={5},
  pages={135-146}
}
@inproceedings{Hu2016DifferentCL,
  title={Different Contexts Lead to Different Word Embeddings},
  author={Wenpeng Hu and Jiajun Zhang and Nan Zheng},
  booktitle={COLING},
  year={2016}
}
@article{Le2014DistributedRO,
  title={Distributed Representations of Sentences and Documents},
  author={Quoc V. Le and Tomas Mikolov},
  journal={ArXiv},
  year={2014},
  volume={abs/1405.4053}
}
@article{Miller1992WordNetAL,
  title={WordNet: a lexical database for English},
  author={George A. Miller},
  journal={Commun. ACM},
  year={1992},
  volume={38},
  pages={39-41}
}
@inproceedings{Leacock1993TowardsBC,
  title={Towards Building Contextual Representations Of Word Senses Using Statistical Models},
  author={Claudia Leacock and Geoffrey G. Towell and Ellen M. Voorhees},
  year={1993}
}
@article{Goodfellow2014GenerativeAN,
  title={Generative Adversarial Networks},
  author={Ian J. Goodfellow and Jean Pouget-Abadie and Mehdi Mirza and Bing Xu and David Warde-Farley and Sherjil Ozair and Aaron C. Courville and Yoshua Bengio},
  journal={ArXiv},
  year={2014},
  volume={abs/1406.2661}
}
@article{Clark2020ELECTRAPT,
  title={ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators},
  author={Kevin Clark and Minh-Thang Luong and Quoc V. Le and Christopher D. Manning},
  journal={ArXiv},
  year={2020},
  volume={abs/2003.10555}
}
@article{Lewis2019BARTDS,
  title={BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension},
  author={Mike Lewis and Yinhan Liu and Naman Goyal and Marjan Ghazvininejad and Abdelrahman Mohamed and Omer Levy and Ves Stoyanov and Luke Zettlemoyer},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.13461}
}
@article{Cohn1994ActiveLW,
  title={Active Learning with Statistical Models},
  author={David A. Cohn and Zoubin Ghahramani and Michael I. Jordan},
  journal={J. Artif. Intell. Res.},
  year={1994},
  volume={4},
  pages={129-145}
}
@article{Guo2019FineTuningBC,
  title={Fine-Tuning by Curriculum Learning for Non-Autoregressive Neural Machine Translation},
  author={Junliang Guo and Xu Tan and Linli Xu and Tao Qin and Enhong Chen and Tie-Yan Liu},
  journal={ArXiv},
  year={2019},
  volume={abs/1911.08717}
}
@inproceedings{Kano2017StructuredBasedCL,
  title={Structured-Based Curriculum Learning for End-to-End English-Japanese Speech Translation},
  author={Takatomo Kano and Sakriani Sakti and Satoshi Nakamura},
  booktitle={INTERSPEECH},
  year={2017}
}
@article{Karras2017ProgressiveGO,
  title={Progressive Growing of GANs for Improved Quality, Stability, and Variation},
  author={Tero Karras and Timo Aila and Samuli Laine and Jaakko Lehtinen},
  journal={ArXiv},
  year={2017},
  volume={abs/1710.10196}
}
@inproceedings{Tran2020SubsetSF,
  title={Subset Sampling For Progressive Neural Network Learning},
  author={Dat Thanh Tran and Moncef Gabbouj and Alexandros Iosifidis},
  year={2020}
}
@article{Berger1996AME,
  title={A Maximum Entropy Approach to Natural Language Processing},
  author={Adam L. Berger and Stephen Della Pietra and Vincent J. Della Pietra},
  journal={Computational Linguistics},
  year={1996},
  volume={22},
  pages={39-71}
}
@article{Chatterjee2017ProgressiveLF,
  title={Progressive Learning for Systematic Design of Large Neural Networks},
  author={Saikat Chatterjee and Alireza M. Javid and Mostafa Sadeghi and Partha P. Mitra and Mikael Skoglund},
  journal={ArXiv},
  year={2017},
  volume={abs/1710.08177}
}
@inproceedings{Bengio2009CurriculumL,
  title={Curriculum learning},
  author={Yoshua Bengio and J{\'e}r{\^o}me Louradour and Ronan Collobert and Jason Weston},
  booktitle={ICML '09},
  year={2009}
}
@inproceedings{Yang2019XLNetGA,
  title={XLNet: Generalized Autoregressive Pretraining for Language Understanding},
  author={Zhilin Yang and Zihang Dai and Yiming Yang and Jaime G. Carbonell and Ruslan Salakhutdinov and Quoc V. Le},
  booktitle={NeurIPS},
  year={2019}
}
@article{Liu2019RoBERTaAR,
  title={RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
  journal={ArXiv},
  year={2019},
  volume={abs/1907.11692}
}
@inproceedings{Radford2018ImprovingLU,
  title={Improving Language Understanding by Generative Pre-Training},
  author={Alec Radford},
  year={2018}
}
@article{Joshi2019SpanBERTIP,
  title={SpanBERT: Improving Pre-training by Representing and Predicting Spans},
  author={Mandar Joshi and Danqi Chen and Yinhan Liu and Daniel S. Weld and Luke Zettlemoyer and Omer Levy},
  journal={ArXiv},
  year={2019},
  volume={abs/1907.10529}
}
@article{Nogueira2019MultiStageDR,
  title={Multi-Stage Document Ranking with BERT},
  author={Rodrigo Nogueira and Wei Yang and Kyunghyun Cho and Jimmy Lin},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.14424}
}
@inproceedings{Wang2018GLUEAM,
  title={GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
  author={Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},
  booktitle={BlackboxNLP@EMNLP},
  year={2018}
}
@article{Clark2019WhatDB,
  title={What Does BERT Look At? An Analysis of BERT's Attention},
  author={Kevin Clark and Urvashi Khandelwal and Omer Levy and Christopher D. Manning},
  journal={ArXiv},
  year={2019},
  volume={abs/1906.04341}
}
@inproceedings{Liu2019LinguisticKA,
  title={Linguistic Knowledge and Transferability of Contextual Representations},
  author={Nelson F. Liu and Matt Gardner and Yonatan Belinkov and Matthew E. Peters and Noah A. Smith},
  booktitle={NAACL-HLT},
  year={2019}
}
@article{Lan2019ALBERTAL,
  title={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
  author={Zhen-Zhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},
  journal={ArXiv},
  year={2019},
  volume={abs/1909.11942}
}
@article{Smith2019ContextualWR,
  title={Contextual Word Representations: A Contextual Introduction},
  author={Noah A. Smith},
  journal={ArXiv},
  year={2019},
  volume={abs/1902.06006}
}
@article{Miller1992WordNetAL,
  title={WordNet: a lexical database for English},
  author={George A. Miller},
  journal={Commun. ACM},
  year={1992},
  volume={38},
  pages={39-41}
}
@article{Athiwaratkun2017MultimodalWD,
  title={Multimodal Word Distributions},
  author={Ben Athiwaratkun and Andrew Gordon Wilson},
  journal={ArXiv},
  year={2017},
  volume={abs/1704.08424}
}
@inproceedings{You2019LargeBO,
  title={Large Batch Optimization for Deep Learning: Training BERT in 76 minutes.},
  author={Yang You and Jing Li and Sashank J. Reddi and Jonathan Hseu and Sanjiv Kumar and Srinadh Bhojanapalli and Xiaodan Song and James Demmel and Kurt Keutzer and Cho-Jui Hsieh},
  year={2019}
}
@article{Mikolov2013EfficientEO,
  title={Efficient Estimation of Word Representations in Vector Space},
  author={Tomas Mikolov and Kai Chen and Gregory S. Corrado and Jeffrey Dean},
  journal={CoRR},
  year={2013},
  volume={abs/1301.3781}
}
@article{Weizenbaum1966ELIZAA,
  title={ELIZA — a computer program for the study of natural language communication between man and machine},
  author={Joseph Weizenbaum},
  journal={Commun. ACM},
  year={1966},
  volume={26},
  pages={23-28}
}
@inproceedings{opennmt,
  author    = {Guillaume Klein and
               Yoon Kim and
               Yuntian Deng and
               Jean Senellart and
               Alexander M. Rush},
  title     = {OpenNMT: Open-Source Toolkit for Neural Machine Translation},
  booktitle = {Proc. ACL},
  year      = {2017},
  url       = {https://doi.org/10.18653/v1/P17-4012},
  doi       = {10.18653/v1/P17-4012}
}
@misc{wang2019jiant,
    author = {Alex Wang and Ian F. Tenney and Yada Pruksachatkun and Katherin Yu and Jan Hula and Patrick Xia and Raghu Pappagari and Shuning Jin and R. Thomas McCoy and Roma Patel and Yinghui Huang and Jason Phang and Edouard Grave and Haokun Liu and Najoung Kim and Phu Mon Htut and Thibault F\'evry and Berlin Chen and Nikita Nangia and Anhad Mohananey and Katharina Kann and Shikha Bordia and Nicolas Patry and David Benton and Ellie Pavlick and Samuel R. Bowman},
    title = {\texttt{jiant} 1.2: A software toolkit for research on general-purpose text understanding models},
    howpublished = {\url{http://jiant.info/}},
    year = {2019}
}
@misc{Rosset2020TNLG,
    author = {Corby Rosset},
    title = {Turing-NLG: A 17-billion-parameter language model by Microsoft},
    howpublished = {\url{https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/}},
    year = {2020}
}
@article{Bahdanau2014NeuralMT,
  title={Neural Machine Translation by Jointly Learning to Align and Translate},
  author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
  journal={CoRR},
  year={2014},
  volume={abs/1409.0473}
}
@inproceedings{Rajpurkar2016SQuAD10,
  title={SQuAD: 100, 000+ Questions for Machine Comprehension of Text},
  author={Pranav Rajpurkar and Jian Zhang and Konstantin Lopyrev and Percy Liang},
  booktitle={EMNLP},
  year={2016}
}
@inproceedings{Vaswani2017AttentionIA,
  title={Attention is All you Need},
  author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  booktitle={NIPS},
  year={2017}
}
@inproceedings{Luong2015EffectiveAT,
  title={Effective Approaches to Attention-based Neural Machine Translation},
  author={Thang Luong and Hieu Pham and Christopher D. Manning},
  booktitle={EMNLP},
  year={2015}
}
@inproceedings{McCallum2000MaximumEM,
  title={Maximum Entropy Markov Models for Information Extraction and Segmentation},
  author={Andrew McCallum and Dayne Freitag and Fernando C Pereira},
  booktitle={ICML},
  year={2000}
}
@article{Peters2018DeepCW,
  title={Deep Contextualized Word Representations},
  author={Matthew E. Peters and Mark Neumann and Mohit Iyyer and Matt Gardner and Christopher Clark and Kenton Lee and Luke Zettlemoyer},
  journal={ArXiv},
  year={2018},
  volume={abs/1802.05365}
}