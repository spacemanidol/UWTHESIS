\chapter {Introduction}
The ability to understand language has long been a fascination for computer scientists worldwide. Early systems like ELIZA \cite{Weizenbaum1966ELIZAA} were designed to interact with humanity through verbal communication. Despite being rudimentary and basic these systems quickly caught the attention of the world and inspired countless researchers to hopefully one day understand and emulate language.  In the decades that have elapsed researchers have grown to understand fundamental concepts about the structure of language and how best to model it. Creating systems that can understand language and preserve its meaning in downstream tasks has long been a focus of the Natural Language Processing(NLP) community. Leveraging deep neural networks researchers have been able to create language representations that represent and understand many of the nuances of human language. These methods have revolutionized nearly every system that works with human language at a pace that seems to be getting faster and faster. These models, despite their impressive performance, are difficult and expensive to create(and recreate) and deploy and it is not clear if there exists a more efficient method to create them \\
In the remainder of this chapter, we first briefly describe the problem we want to tackle and how we will tackle these problems in Section 1.1. Then, we give a brief overview of Curriculum Learning and Language Representations in Section 1.2 and 1.3 respectively. We briefly discuss the goals and challenges of this dissertation in sections 1.4. We then summarize the contributions of this dissertation in section 1.5. We then provide a brief outline of the Dissertation structure in section 1.6 before presenting our statement of originality in section 1.7.
\section{Overview}
This dissertation addresses the topic of the effect of Curriculum Learning methods on Language Models. The term 'Curriculum Learning' refers to a training procedure where the training data for a machine learned method is constructed with a deliberately assembled order with the goal of steering training to a more optimal solution sooner. The term 'Language Modeling' refers to a variety of statistical representations which are used to represent textual language. These methods can be used to understand language and represent language for downstream Natural Language Processing tasks like Question Answering and Information Retrieval. In this dissertation we will explore how curriculum learning methods can be leveraged to improve and create language representations.
\section{Curriculum Learning}
Curriculum's have long been part of the way humans learn. In our schooling instructors assemble information and instruction in a way that allows students to learn foundations and simpler concepts first before they move onto difficult samples. In traditional machine learning for each step of the network learning a portion of the training data is sampled at random. Unlike students in school networks must learn to understand difficult examples at the same time it learns to understand simple examples. There are many formalization's and implementations for curriculum like methodologies in machine learning but for the purpose of this dissertation we will focus on the formalization in frame of neural networks by Bengio et al 2009 \cite{Bengio2009CurriculumL}. In their formalization, curriculum learning is a method for altering the distribution of the training data to allow the network to simpler concepts before difficult concepts.
\section{Representing Language}
In the frame of language understanding researchers have long focused on how to represent all the information which text represents in a way which can be used by other systems. Researchers have explored human curated representations like wordnet \cite{Miller1992WordNetAL}  and explore statistical methods to represent corpuses \cite{Leacock1993TowardsBC}. More recently, statistical methods have tried to produce vector space models where words are represented by a set of vectors in an n-dimensional plane. In the early 2010s, methods like Word2Vec \cite{Mikolov2013EfficientEO} and Glove \cite{Pennington2014GloveGV} were introduced, and the broader NLP community found great leverage in vector based representations of language. Since these word representations mapped each word to a higher dimensional numeric representation systems could now take advantage of the closeness of words via methods like cosine similarity. Moreover, these vector representations of words proved to be a powerful method to represent language for many downstream tasks like: Information Retrieval, Question Answering, Sentiment Analysis, Machine Translation, etc. \\ 
These fixed word vectors drove incredible improvement and research across many fields but since representations focused on word level vector values, systems were unable to represent word as it different in context. For example: the word fly was represented by the same vector independent if the context was dealing with an insect (There is a fly in my soup!), an action (I want to fly away.), or a description of personal style(You sure look fly!). To expand word representation out of discrete words researchers explore methods like having multiple word vectors for each meaning of a word \cite{Hu2016DifferentCL}, sentence vectors \cite{Kiros2015SkipThoughtV}, paragraph vectors \cite{Le2014DistributedRO}, Gaussian mixtures \cite{Athiwaratkun2017MultimodalWD}  and sub word vectors \cite{Bojanowski2017EnrichingWV} all with the goal of improving models ability to capture expressive semantic information and performs better in situations with polysemy. Many of these techniques were very similar to word vectors but tried to represent all sentences in a corpus using a fixed size vector. While these techniques also drove vast improvement in NLP tasks they struggled to represent sentences outside of the original training corpus and minor changes in sentence structure could have a large effect on the vector representation. \\
As part of these explorations, researchers used Language Models(LM) to learn word representations. Language modeling is a well defined NLP problem that aims to model a statistical distribution of words in a sentence so we can for some type of prediction about next word. Using LM systems with sesame street inspired names like: ELMO \cite{Peters2019KnowledgeEC}, BERT \cite{Devlin2019BERTPO}, ERNIE \cite{Sun2019ERNIE2} were able to produce what could seem like a never ending stream of new State-of-the-art(SOTA) models that excelled at most NLP problems. The success of these models spun a seemingly continuous wave of new techniques building of BERT like KnowBERT \cite{Peters2019KnowledgeEC}, RoBERTa \cite{Liu2019RoBERTaAR}, AlBERT \cite{Lan2019ALBERTAL}, ELECTRA \cite{Clark2020ELECTRAPT}, and BART \cite{Lewis2019BARTDS}.  \\
While these deep learning based LMs have shown to be great methods to enable language understanding in many tasks, the ability to train these models is becoming increasingly computationally expensive. 
\section {Goals and Challenges}
The goal of this dissertation is to explore how curriculum learning effects Language representations. First we explore methods for evaluating the difficulty of a sentence. Using difficulty scores we then explore how to use this difficult to develop and train models with a curriculum. The final of this dissertation is to evaluate the wide spread of curriculum methods in a way that we can draw conclusions which are representative for usage on all language models.
The main challenge in this dissertation is to scope the work to allow us to gain maximum feedback cycles. Since training large language models requires a lot of computation doing an exhaustive exploration of both curriculum methods and tuning doing an intensive hyperparameter sweep for each model would be too time intensive. Ensuring our work is representative to variations in model size and dataset size will also be challenging. Finally, ensuring our work is reproducible and expandable will also be a challenge.ks. 
\section{Contribution}
None yet
\section{Structure of This Dissertation}
In Chapter 2 we present the background material in system architecture Curriculum learning, Language Modeling, and methods for evaluating Language Models. For Curriculum learning, we give a detailed and technical introduction to Curriculum Learning and discuss some recent applications in the NLP domain. With regard to Language modeling, we introduce the concept of language modeling, describe various methodological advances and improvements discovered over time, and contrast various implementations. We will also provide a brief introduction to the effects of large language models and why focusing on model training efficiency is important. Finally, we introduce some of the most popular methods for evaluating language modeling along with the datasets and frameworks we will use for our experiment. \\
In Chapter 3, we provide a detailed explanation of our experimental setup. We propose two curriculum learning strategies building on prior work and explain how we implemented these strategies and how we evaluate the effectiveness. 
In Chapter 4, we discuss the results of our experiments along with a broad analysis of the impact our various methodologies had on model performance.
In Chapter 5, we conclude this thesis.
In Chapter 6, we discuss future work we would like to research.
\section{Statement of Originality}
I declare that this thesis was composed by myself and that the work it presents is my own, except where otherwise stated.
