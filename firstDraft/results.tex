\chapter{Results}
\subsection{Competence Based Curricula}
\begin{table}[h!]
\begin{tabular}{|l|l|} \hline
\textbf{Model} & \textbf{wt103v perplexity} \\ \hline
Megatron-LM & 10.8 \\ \hline
GPT-2 Full & 17.4 \\ \hline
GPT-2 Large & 22.05 \\ \hline
GPT-2 Small & 37.50 \\ \hline
LSTM & 29.2 \\ \hline
\end{tabular}
\caption{Top Systems on Wikitext103}
\label{table:top103}
\end{table}
from \footnote{https://www.paperswithcode.com/sota/language-modelling-on-wikitext-103}
\section{Effect of Training Length}
We explore the effect of training until no change in perplexity vs fixed 10 epoch.  \ref{table:effectoftrainlen}
\begin{table}[h]
\tiny
\begin{tabular}{|l|l|l|l|l|} \hline
\textbf{Corpus} & \textbf{Train Method} & \textbf{Batches Trained} & \textbf{wikitext-2 validation perplexity} & \textbf{wikitext103 validation perplexity} \\ \hline
wikitext2 & Converge & 11669 & 376.79047  & 389.78775 \\ \hline
enwiki-2 & Fixed & 2700  & 143.46458 & 141.86278 \\ \hline
enwiki-103 & Converge & 396192 & 28.56237 &  33.93132 \\ \hline
enwiki-103 & Fixed & 132075 & 36.15289 & 36.412056  \\ \hline
\end{tabular}
\label{table:effectoftrainlen}
\end{table}
\section{Effect of Sentences vs lines}
We explore the effect sentence level training \ref{table:effectofsentvsline}
Trained until loss doesn't improve for 1000 batches
\begin{table}[h!]
\tiny
\begin{tabular}{|l|l|l|l|l|l|l|} \hline
\textbf{training corpus} & \textbf{wikitext2} & \textbf{wikitext2} & \textbf{wikitext2}  & \textbf{wikitext103} & \textbf{wikitext103} & \textbf{wikitext103} \\ 
& \textbf{sentence vp} & \textbf{line vp} & \textbf{average vp}  & \textbf{sentence vp} & \textbf{line vp} & \textbf{average vp} \\ \hline
wikitext2 & 712.61896 & 376.79047 &544.704715 &  736.8754 & 389.78775 & 563.331575\\ \hline
wikitext2-sent & 533.29083 & 337.12128 & 435.206055&  325.10345 & 548.88055 & 436.992 \\ \hline
wikitext103 & 58.68944 & 28.56237 & 43.625905 & 75.65022 & 33.93132 & 54.79077\\ \hline
wikitext103-sent & 38.20821 & 48.146133& 43.1771715& 46.631134 & 60.21795 & 53.424542 \\ \hline
\end{tabular}
\caption{Validation Perplexity(VP) of various training regimes}
\label{table:effectofsentvsline}
\end{table}
\section{Language Modeling}
use line and sentence
\subsection{Corpus Modification Curricula}
\begin{table}[h!]
\tiny
\begin{tabular}{|l|l|l|l|} \hline
\textbf{Train Corpus} & \textbf{train method} &  \textbf{wikitext2 valid perplexity} & \textbf{wikitext103 valid perplexity} \\ \hline
wikitext2 & standard & 151.25607 & - \\ \hline
wikitext2 & curriculum & 153.26689 & - \\ \hline
wikitext103 & standard & - & 36.412056 \\ \hline
wikitext103 & curriculum & - & 37.029747 \\ \hline
\end{tabular}
\label{table:corpuscurriculaperplexity}
\end{table}
\begin{table}[h!]
\tiny
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|l|l|} \hline
\textbf{Model} & \textbf{Train Corpus} & \textbf{COLA} & \textbf{SST-2} & \textbf{MRPC} & \textbf{STS-B} & \textbf{QQP}& \textbf{MNLI} & \textbf{QNLI} & \textbf{RTE} & \textbf{WNLI} & \textbf{AX} \\ \hline
ELMO \cite{Smith2019ContextualWR} & 1B Word Benchmark & 0.000 & 0.852 &  0.767 & 0.547/0.562 & 0.749 & 0.671 & 0.719 & 0.480 & 0.563 & 0.155 \\ \hline
baseline & wikitext103 & 0.281 & 0.862 & 0.834 & 0.765/0.773 & 0.739& 0.644 & 0.761 &  0.610 & 0.535 & 0.139 \\\hline
corpus modification & wikitext103 & 0.254 & 0.852 & 0.845 & 0.794/0.793 & 0.761 & 0.662 & 0.719 & 0.588 & 0.437 & 0.162 \\ \hline
baseline & wikitext2 & 0.000 & 0.700 & 0.810 & 0.661/0.663 & 0.727 & 0.585 & 0.717 & 0.542 & 0.563 & 0.130 \\ \hline
corpus modification & wikitext2 & 0.060 & 0.742 & 0.821 & 0.683/0.684 & 0.721 & 0.566 & 0.726 & 0.581 & 0.563 & 0.119 \\ \hline
\end{tabular}
\label{table:corpuscurriculaGLUE}
\end{table}
major movers MRPC, STS-B, wNLI
\subsection{Competence Curricula}
\begin{table}[h!]
\tiny
\begin{tabular}{|l|l|l|l|} \hline
\textbf{Train Corpus} & \textbf{train method} &  \textbf{wikitext2 valid perplexity} & \textbf{wikitext103 valid perplexity} \\ \hline
wikitext2 & true baseline & 533.29083 & 325.10345 \\ \hline
wikitext2 & competence baseline & 770.4932 & 769.72424 \\ \hline
wikitext2 & random & 435.65863 & 428.7367 \\ \hline
wikitext2 & length & 540.70087 & 535.1956 \\ \hline
wikitext2 & dependency parse & 310.6444 & 305.38766 \\ \hline
wikitext2 & POS & 464.69473 & 460.49887 \\ \hline
wikitext2 & trigram & 414.7562 & 412.121 \\ \hline
wikitext2 & bigram & - & -\\ \hline
wikitext2 & unigram & 671.31067 & 669.3966 \\ \hline
wikitext2 & selftutoring & - & -  \\ \hline
wikitext2 & ensemble & - & \\ \hline
\end{tabular}
\label{table:corpuscurriculaperplexity}
\end{table}
Glue Results
\begin{table}[h!]
\tiny
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|l|l|} \hline
\textbf{Model} & \textbf{Train Corpus} & \textbf{COLA} & \textbf{SST-2} & \textbf{MRPC} & \textbf{STS-B} & \textbf{QQP}& \textbf{MNLI} & \textbf{QNLI} & \textbf{RTE} & \textbf{WNLI} & \textbf{AX} \\ \hline
ELMO \cite{Smith2019ContextualWR} & 1B Word Benchmark & 0.000 & 0.852 &  0.767 & 0.547/0.562 & 0.749 & 0.671 & 0.719 & 0.480 & 0.563 & 0.155 \\ \hline
baseline & wikitext2 & 0.000 & 0.700 & 0.810 & 0.661/0.663 & 0.727 & 0.585 & 0.717 & 0.542 & 0.563 & 0.130 \\ \hline
competence baseline & wikitext2 &  \\ \hline
random & wikitext2 & 0.000 & 0.749 & 0.797 & 0.691/0.689 & 0.734 & 0.552 & 0.712 & 0.578 & 0.563 & 0.118  \\ \hline
dependency parse & wikitext2 & 0.181 & 0.750  & 0.821 & 0.678/0.676 & 0.750 & 0.614 & 0.748 & 0.588 & 0.563 & 0.128 \\ \hline
POS & wikitext2 & \\ \hline
trigram & wikitext2 & \\ \hline
bigram & wikitext2 & \\ \hline
unigram & wikitext2 & \\ \hline
selftutoring & wikitext2 & \\\hline
ensemble & wikitext2 & \\ \hline
baseline & wikitext103 & 0.281 & 0.862 & 0.834 & 0.765/0.773 & 0.739& 0.644 & 0.761 &  0.610 & 0.535 & 0.139 \\\hline
competence baseline & wikitext103 &  \\ \hline
random & wikitext103 & \\ \hline
dependency parse & wikitext103 & \\ \hline
POS & wikitext103 & \\ \hline
trigram & wikitext103 & \\ \hline
bigram & wikitext103 & \\ \hline
unigram & wikitext103 & \\ \hline
selftutoring & wikitext103 & \\\hline
ensemble & wikitext103 & \\ \hline

\end{tabular}
\label{table:corpuscurriculaGLUE}
\end{table}
\section{Discussion}
Discuss results here