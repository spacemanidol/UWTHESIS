\chapter{Research Approach}
To understand the effects that curriculum learning can have on LM we will explore the effect various curriculum methods have on training, evaluation, and usage in downstream tasks. Our methodology is driven around three main research questions:
\begin{enumerate}
\item Does curriculum learning help a Language Model converge to a more optimal global minimum? 
\item Does the language representation learned via curriculum learning improve performance on downstream tasks when compared to non curriculum methods?
\item  Do curriculum learning methods increase model convergence speed in both pre-training and in downstream task fine-tuning?
\end{enumerate}
In this chapter we will first describe the structure of our experiment as scoped to various implementations of curriculum learning. Third, we will describe and discuss our methods for curriculum development. Fourth we will discuss how we will train our Language models and how we will explore hyperparameter tuning. Finally, we  discuss how we evaluate and compare the effect of all of our curriculum's.
\section{Experiment Structure}
Our experimentation strategy is simple: train many models with fixed structure and hyper parameters using different curricula. Once a robust set of models have been trained all models are evaluated on the held out portion of the training corpus and on downstream transfer tasks.
\subsection{Datasets}
For our training corpus we leverage two well established language modeling benchmarks of wikitext-2 and wikitext-103 \cite{Merity2016PointerSM}. Each corpus has already been tokenized and processed and split into train, validation, and evaluation components(80:10:10 based on splitting the source documents). All tokens that occur less than 3 times are replaced with a $<UNK>$ token to represent unknown words. While most other research on Language modeling has been focusing on bigger and bigger data our focus on smaller corpuses allow us to experiment with more methods in curriculum creation. We understand this will limit our model performance relative to current top methods as smaller corpus limits the model performance \cite{Kaplan2020ScalingLF}. Moreover, these corpuses were chosen because they are standard benchmarks for language modeling and of sufficient size to train large scale language models. We believe that the diversity of our two corpuses (103 is 50x larger than 2) will allow us to draw broad conclusion about curriculum learning independent to corpus size. More information about corpus size and diversity can be found in table \ref{table:1}. Information about the corpus size used for other LM is included for comprehensiveness \cite{Chelba2014OneBW}.\\
\begin{table}[h!]
\begin{tabular}{|l|l|l|l|l|} \hline
\textbf{Corpus Name} & \textbf{vocabulary Size} & \textbf{Tokens} & \textbf{lines} & \textbf{sentences} \\ \hline
enwiki-2 & 33278 & 2507007 & 44836 & 131262 \\ \hline
enwiki-103 & 267735 & 103690236 & 1809468  & 5343947 \\ \hline
1B Word Benchmark & 793471 & 829250940 & 0 & 0 \\ \hline
\end{tabular}
\caption{Training Corpus details}
\label{table:1}
\end{table}
\subsection{Language Model}
To optimize how quickly we can train our systems we will only explore the effect of curriculum on an established successful baseline, ELMO \cite{Smith2019ContextualWR}. We leverage the code used for the original Elmo experiments and keep use the same hyper parameters reported in their paper. We make two changes to their implementation. First we change the corpus input format from lines to sentences. We do this as it simplifies our usage of curriculum methods. Second, we introduce a method to specify a difficulty score for each sentence and then implement a curriculum using these difficult scores. \\
Our baselines are regular ELMO models trained using each of our corpuses. For each corpus we train 4 models: fixed epochs(10 epochs) on line based corpus, fixed epochs(10 epochs) on sentence based corpus, until convergence(no change in loss in 1000 batches) on line based corpus, and until convergence(no change in loss in 1000 batches) on sentence based corpus. Each model was trained using 3 Nvidia 2080 Ti GPUs and estimated cost to train is included with each result. Our code and implementation has been published online \footnote{https://github.com/spacemanidol/ProgressiveLanguageLearning}. 
\subsection{Evaluation}
To evaluate our models we will focus on three aspects of LM performance: performance on the trained task, performance on a downstream task, and speed of convergence. \\
The broad goal of our language models is to accurately represent a given corpus and to do so we will evaluate model perplexity on the held out portions of our datasets.  on each of our datasets. The broad goal of our language models is to accurately represent a given corpus In other words for each model we will produce 4 perplexity scores: valid-103, eval-103, valid-2, eval-2. \\
To measure the quality of our Language Representation downstream we will use the industry standard GLUE task. As previously described in our related works section, GLUE(General Linguistic Understanding Evaluation) is a collection of 11 natural language evaluation benchmarks ranking for question answering to sentiment detection. This is a standard benchmark for evaluation of language representation and provides a good measure of improvements and magnitude of improvement relative to other systems.  
\section{Curriculum Construction Methods}
In our experiments we first build a broad set of baselines and then we explore two implementations of curriculum learning: corpus modification and competence based data selection. 
\subsection{Baseline}
Before we study the effect of curriculum learning on language modeling we first produce a broad set of baselines to ensure we observe the effects of curriculum learning not just changes in experiment formalization. We build baseline methods without curriculum for each of our datasets using line based corpus and sentence based corpuses and in each method we train for both a fixed window of 10 epochs and until model perplexity does not improve after 1000 batches. 
\subsection{Corpus modification}
Corpus modification follows the implementation covered in the Begio et al. 2009 \cite{Bengio2009CurriculumL} implementation of curriculum learning. In their implementation they trained their model as an Auto regressive model. The curriculum they used is based on vocabulary occurrence. In the first stage of the curriculum the model trains one full epoch(the entire dataset) but all words that are not part of the 5,000 most common are ignored. For the second stage, the corpus is kept constant but only words that are not part of the 10,000 most common are ignored. This process of vocabulary expansion is continued until the model is able to train on the entire vocabulary. In the original implementation the authors see improved model performance and quicker convergence in later stages. \\
In our implementation, we apply this strategy to our two corpuses and generate 7 files per corpus. When a word does not meet the vocabulary threshold are replaced with the $<UNK>$ token. Using these files we train on each for one full epoch and train for an additional 4 epochs on the full vocabulary file. The relative vocabulary size for each of these six splits is corpus dependent with the breakdowns for our dataset found in \ref{table:2}. Note the vocabulary 
\begin{table}[h!]
\begin{tabular}{|l|l|l|l|l|l|l|l|} \hline
\textbf{Corpus Name} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7-*}  \\ \hline
enwiki-2 & 5,000 & 10,000 & 15,000 & 20,000 & 25,000 & 30,000 & 33,278 \\ \hline
enwiki-103 & 25,000 & 50,000 & 75,000 & 100,000 & 150,000  & 250,000 & 267,735\\ \hline
\end{tabular}
\caption{Curriculum Method Vocabulary per epoch}
\label{table:2}
\end{table}
\subsection{Competence Method}
Following the work of \cite{Platanios2019CompetencebasedCL} we will apply the notion of a competence based curriculum to language model training. As covered in depth in our prior work section, competence based methods rely on the ability to assign a difficulty score to each sample in the training corpus and use this to only allow the model to train on problems that are easier than its current competence level. A models competence score is defined by how far along in a training regime the model is. Like the original paper, we set the training length to 90\% of the batches required to have the model achieve its minimum entropy: batches for enwiki-8, batches for enwiki-103. At each step in training model competence is represented by $\lambda$ where $\lambda_0 = 0.01$ and $\lambda_t = $.  Prior to training each example in the training data has been assigned a difficulty score from 0 to 1 represented by $\epsilon$. Difficulty scores are assigned by one of the 8 heuristics will shortly describe. For each training batch, the model is only able to train on samples that have a difficulty we only Then during training a batch can only be made up of samples where $\epsilon <= \lambda$. \\
To produce a dataset which is consumable by the competence method our training data needs to be broken into available units, in this dissertation we focus on sentence level data. In the rest of this section we denote our
training corpus, $X$ as a collection of sentences $s_1$, where each sentence is a sequence of words $s_i= w_o^i,w_1^i,...,w_n^i$. 
To produce a difficulty score for each sample we must first define a heuristic which we can use to sort our corpus. To explore what methods produce the greatest improvements in language models we will sort our training data via:self tutoring, sentence length, unigram sentence entropy, bigram sentence entropy, trigram sentence entropy, part of speech diversity, sentence dependency complexity, and a linear combination of the all other methods. For each methodology, for each $s_i$ in $X$ we compute a difficulty value $\epsilon$. Once the entire corpus is annotated we compute the cumulative density function(CDF) of the difficulty score per sample $s_i\epsilon \in [0,1]$. This method is formalized in Algorithm \ref{algo:competence}.\\
\begin{algorithm}[H]
\label{algo:competence}
\SetAlgoLined
\KwResult{Model Trained with Competence Based Curricula}
Compute difficulty, $s_i\epsilon$ for $s_i \in X$\;
Compute Cumulative density function of $s_i\epsilon$\;
Input: Dataset, X = {$s_i$}\;
\For{training step t = 1,...,n}{
Compute model competence $\lambda_t$\;
Sample a data batch $b$ from X such that $s_i\epsilon < \lambda_t$\;
Train on batch $b$
}
\caption{Competence-based curriculum}
\end{algorithm}
\subsection{Training configurations}
train wikitext2 for 3823 batches
train wikitext 103 for 188960 batches
with batch size of 128(3 gpus) so 10 epochs
wikitext 103 initial competence 0.001 with competence increment 1e-5
wikitext 2 initial competence of 0.1 and competence increment 0.0004
\subsubsection{Self Tutoring}
Self tutoring is defined by using an existing model to define the difficulty of each sentence based on the perplexity the already trained model achieves for each sentence. While in theory any existing model could be used, we will use the perplexity scores from our non curriculum training of ELMO on enwiki-103. For each sentence $s_i \in X$ the  $s_i\epsilon = perplexity_i$.
\subsubsection{Sentence Length}
We argue that is is a lot harder to model longer sentence, as longer sentences require better tracking of dependencies. We believe this method would be particularly effective in transformer based models as it can steer the model into learning how to leverage its multi-headed attention with different sentence lengths. For each sentence $s_i \in X$ the  $s_i\epsilon = length(s_i)$.
\subsubsection{Sentence Entropy}
Another part of language that can be difficult to model is words with a variety of frequency in the corpora. Models, if assumed to behave like humans, would find it difficult to understand the meaning of a word if they do not see it in a corpus nor have a diversity of usages to infer meaning. Since the statistical strength of training samples with rare words is low and the learning word embeddings are likely to have high variance it is likely that exposing a model early to rare words can result in over and underestimation of true gradients and make convergence difficult. To quantify this difficulty we propose producing a sentence entropy for each sentence with respect to its unigram, bigram, and trigram probabilities. These products can be thought of as an approximate naive language modeling as it assumes words are sampled independently and also implicitly incorporates information as sentence length as the unigram probability decreases and sentence length grows. Note, we are not calculating the conditional probability of each word given the preceding n words but the probability of the n-gram given the text corpus. To produce a difficulty score for each sentence $s_i$ we first calculate a n-gram probability for each unigram equation \ref{equation:unigramprob}, bigram equation \ref{equation:bigramprob}, and trigram equation \ref{equation:trigramprob}in the training corpus. For n-gram difficulty, we calculate the product of all the n-grams in the sentence. For unigrams please see equation \ref{equation:unigramdifficulty}, bigrams please see  equation \ref{equation:bigramdifficulty}, and trigrams please see.  equation \ref{equation:trigramdifficulty}. 
\begin{equation}
    p(w_i) = \sum_{m=0} ^ {length(X)} \sum_{n=0}^{length(s_m)} w_i = w_{nm}
\label{equation:unigramprob}
\end{equation}
\begin{equation}
    p(w_{i}, w_{i+1}) = \sum_{m=0} ^ {length(X)} \sum_{n=0}^{length(s_m)-1} w_i, w_{i+1} = w_{mn},w_{mn+1}
\label{equation:bigramprob}
\end{equation}
\begin{equation}
    p(w_{i}, w_{i+1}, w_{i+2}) = \sum_{m=0} ^ {length(X)} \sum_{n=0}^{length(s_m)-2} w_i, w_{i+1},w_{i+2} = w_{mn},w_{mn+1},w_{mn+2}
\label{equation:trigramprob}
\end{equation}
\begin{equation}
   s_i\epsilon = \prod_{n=0}^{length(s_i)} log(p(, w_n))
\label{equation:unigramdifficulty}
\end{equation}
\begin{equation}
        s_i\epsilon = \prod_{n=0}^{length(s_i)-1} log(p(w_{n-1}, w_n))
\label{equation:bigramdifficulty}
\end{equation}
\begin{equation}
    s_i\epsilon = \prod_{n=0}^{length(s_i)-2} log(p(w_{n}, w_{n+1}, w_{n+2}))
\label{equation:trigramdifficulty}
\end{equation}
\subsubsection{Sentence Complexity}
There are various methods to define sentence complexity but in our experiments we scope complexity to the complexity of a dependency parse. We leverage the language processing framework spacy \footnote{spacy.io} and for each sentence we generate a dependency parse and starting at the root we measure the depth of the tree. This formalizes sentence difficulty for  $s_i \in X$ the  $s_i\epsilon = depth(s_i)$.
\subsection{Part Of Speech Diversity}
Another core part of language complexity can be derived by the diversity of parts-of-speech in a sentence. We believe that more difficult sentences feature a higher diversity of parts-of-speech. We leverage the part of speech parser from spacy to produce a set of all pos in each sentence. Using this we formalize sentence difficulty for $s_i \in X$  where $pos_{s_i}$ represents the set of all pos in $s_i$ as $s_i\epsilon = len(pos_{s_i})$.
\subsubsection{Ensemble Method}
Our ensemble method relies on a linear combination of the $\epsilon$ value of each of the prior heuristics. Each method receives equal weight and as such, this heuristic is formalized in \ref{equation:ensemble} where $s_i\epsilon_t$ is the self tutoring difficulty, $s_i\epsilon_l$ is the sentence length difficulty, $s_i\epsilon_u$ is the unigram difficulty, $s_i\epsilon_b$ is the bigram difficulty, $s_i\epsilon_r$ is the trigram difficulty, $s_i\epsilon_c$ is the complexity difficulty, and $s_i\epsilon_p$ is the part of speech difficulty.
\begin{equation}
\label{equation:ensemble}
s_i\epsilon =\frac{1}{7}s_i\epsilon_t +\frac{1}{7}s_i\epsilon_l +\frac{1}{7}s_i\epsilon_u+\frac{1}{7}s_i\epsilon_b +\frac{1}{7}s_i\epsilon_r +\frac{1}{7}s_i\epsilon_c  +\frac{1}{7}s_i\epsilon_p
\end{equation}

