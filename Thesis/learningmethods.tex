\section{Learning Methods}
\label{chap:prior:sec:cl}
In this section, we first briefly review some learning methodologies for neural networks in \fullref{chap:prior:sec:cl:overview} to formalize what learning methods seek to achieve. Then, in \fullref{chap:prior:sec:cl} we provide a more in-depth description of curriculum learning and cover some ways it has been using in NLP. 
\subsection{Brief Overview of learning methodologies}
\label{chap:prior:sec:cl:overview}
Neural Networks are usually trained by randomly selecting batches of data in a training corpus. This method has proven to be incredibly robust as it allows the model to learn the data distribution gradually. While incredibly useful, random sampling is unable to build any natural hierarchy or structure quickly. Methods like Curriculum Learning \cite{Bengio2009CurriculumL}, reinforcement learning \cite{Sutton1998ReinforcementLA}, and active learning \cite{Cohn1994ActiveLW} are alternative methods which try to improve model training and accuracy by sampling training examples in a non-random way. Most methods at their core seek to optimize what kind of information a model has access to at each step in training to find better gradients than a random sample. In domains like Generative Adversarial Networks (GAN) \cite{Goodfellow2014GenerativeAN}, training models that generate large images has proven difficult. Finding structure in the size of images, researchers have found tremendous improvements by slowly increasing the target output size as training progresses \cite{Karras2017ProgressiveGO}. Initially, the Generator produces 2x2 pixel images. Once it and the Discriminator converge, the target output size is increased to 4x4. This method of scaling continues until 4096x4096 pixel images are synthesized. By training in an increasingly entropic way, the final model can learn a better representation with a higher sample efficiency. This work on progressive learning in GANS inspired this dissertation as LMs, much like GANs, are conceptually simple but can prove difficult to train at scale. 
\subsection{Curriculum Learning}
\label{chap:prior:sec:cl:cl}
While the common usage of CL in computer science begins in 2009 \cite{Bengio2009CurriculumL}, the concept of CL is much older. At its core, CL's vision is the idea of choosing examples to be presented in a specific order to guide the agent to learn quicker than if they had seen samples in random order. Early experiments with RNNs \cite{Elman1993LearningAD} focused on learning grammar suggested that learning of complex grammatical structure improves when the initial examples the models learn with are more straightforward. CL guides the optimization process to converge faster and guide the learner to better local minima and can be thought of as a method of re-weighting the data distribution over model training. Unlike CL, regular random batch sampling emphasizes an equal contribution of each data point without any notion of how common the data point is and if the data point can be used to build a foundational understanding.\\
In the original paper, the authors suggest that CL approaches training may act similarly to unsupervised pretraining. The authors explore the effect of CL in three experiments: using a perceptron to learn an equation, shape recognition, and language modeling. In their language modeling task, they modify the training corpus to make it increasingly difficult. Starting with a 631m token Wikipedia corpus, they initially replace any word that is not in the N most common words (starts with 5,000) with a token to represent unknown words, $<UNK>$. After each pass on the corpus, N is increased by 5,000. After 1 billion updates, the CL method has a loss of 2.78 vs. the non-CL loss of 2.83. The two main issues in CL are: the computational cost to assemble the batch and the significant amount of data the model can learn from in early epochs is low. \\
Since this original paper, CL methods have proven effective for many NLP domains, especially in NMT. Wang et al., 2019 \cite{Wang2019LearningAM} expand on the idea of CL as a method of data selection and data augmentation. Their implementation focuses on selecting data relevant to all tasks and disregarding data that may be only applicable to a specific domain and can bring a 2.5 BLEU point improvement vs. non-curricula implementation. Platanios et al., 2019 \cite{Platanios2019CompetencebasedCL} introduce the notion of competence-based CL, which is the basis for much of our experimentation. The author's main contribution is building a CL method called competence curricula, which only controls how long the curriculum lasts before regular training occurs. The authors' approach is 2-stepped: assign a difficulty value to each sample in the training data, and train the model with increasingly more data as its competence improves. In the first stage, a heuristic is applied to rank the training data from easiest to hardest. Using a cumulative density function (CDF), each sample is then given a value from 0 to 1, which equates to how difficult the example is. Then, starting with some initial competence $\lambda_0$, the model will train by sampling a training batch from the training data where the sample difficulty is lower than the model's current competence. After each batch is sampled, the model's competence is increased by some step $\epsilon$ until it is training on the full dataset. A more detailed description as it applies to this dissertation can be found in \fullref{chap:method}. In their experiments on NMT, the authors explore the effect of competence-based CL using Transformers and BiLSTMs using two difficulty methods(sentence length and word rarity) and two competence step functions(root and linear) and find that all of their CL implementations outperform their non-CL counterparts. Using the competence curricula method, the authors can reduce training time by up to 70\% and improve BLEU performance by 2.2 points. 