\chapter{Conclusion}
\label{chap:conclusion}
Throughout this dissertation, we have explored how some approaches in curriculum learning apply to language modeling. Our research explored two types of curricula and used them to the pretraining of ELMo, a language model. Our first curricula (previously referred to as Bengio Style) makes the training corpus more difficult by limiting the training vocabulary in regularly spaced increments. This curriculum does not improve model perplexity on the training corpus, but when the corpus is small outperforms the non-curricula methods on transfer tasks. Our second curricula method (previously referred to as competence curricula) explores the effect of various curricula when applied to the same language model pretraining. In these experiments, we find that while the model cannot learn a good representation of the training corpus, their representations transfer well to downstream NLP tasks. We find that on small datasets, competence curricula show improvement versus non-curricula method across the board. As we scale the corpus size, we find that non-curricula methods outperform all curricula. We do not see any superiority in the curriculums we explore, nor do we find a clear difference in training effects with sentences over lines. While our implementations could not produce improvements, we believe the results set the stage for further research and pose some broader questions for learning methods for NN in general. 