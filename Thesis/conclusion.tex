\chapter{Conclusion}
\label{chap:conclusion}
Throughout this dissertation, we have explored how some approaches in curriculum learning apply to language modeling. Our research explored two types of curricula and used them to pretrain ELMo, a language model. Our first curriculum, corpus replacement style, introduces no compute overhead in training and makes the training corpus more difficult by limiting the training vocabulary in regularly spaced increments. This curriculum does not improve model perplexity on the training corpus, but when the corpus is small outperforms the non-curriculum methods on transfer tasks. Our second curriculum, competence based curriculum, explores the effect of various curricula when applied to the same language model pretraining. In these experiments, we find that while the model cannot learn a good representation of the training corpus, their representations transfer well to downstream NLP tasks. We find that on small datasets, competence curriculum show improvement versus non-curriculum methods across the board. As we scale the corpus size, we find that non-curriculum methods perform best. We do not see any superiority in the curriculums we explore, nor do we find a clear difference in training effects with sentences over lines. While our implementations could not produce improvements, we believe the results set the stage for further research and pose some broader questions for learning methods for NN. 