\chapter{Conclusion}
Throughout this thesis we have explored how some approaches in curriculum learning apply to language modeling. In our research we explored 2 types of curricula and applied them to the pretraining of ELMO, a language model. Our first curricula(previously referred to as Bengio Style) makes the training corpus more difficult by limiting the training vocabulary in regularly spaced increments. This curricula is not able to improve model perplexity on the training corpus but when the corpus is small outperforms the non curricula methods on transfer tasks and as the corpus scales improves the performance on a subset of NLP applications. Our second curricula method(previously referred to as competence curricula) explores the effect of various curricula when applied to the same language model pretraining. In these experiments we find that while the model cannot learn a good representation of the training corpus the representations they transfer downstream NLP tasks prove to be resilient. We find that on small corpuses competence curricula show improvement versus non curricula method across the board. As we scale the corpus size we find that non curricula methods out perform all curricula. We do not find any superiority in the curricula we explore nor do we find a compelling difference in the effects of training with sentences over lines. While our implementations were not able to produce improvements we believe the results set the stage for further research and poses some broader questions for learning methods for NN in general. 