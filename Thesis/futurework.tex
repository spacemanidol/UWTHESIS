\chapter{Future Work}
In the future we seek to continue our exploration in CL methods and methods to make training and using models more efficient. Since we began our work there has been a lot of exciting work on finding sub networks in NN that preserve model accuracy \cite{Frankle2019TheLT} and the pruning of large models for smaller equally accurate models \cite{Han2016DeepCC} \cite{Yu2017ScalpelCD} \cite{Wynter2020OptimalSE} which we would like to study focusing on how these pruning methods perform with transfer learning. Additionally, inspired by the layered nature of transformer based models we would like to explore jointly increasing transformer depth while increasing data difficulty similar to what has been done with GANs \cite{Karras2017ProgressiveGO}. Finally, in the future we seek to make a benchmarking system that can allow authors to explore the effect of various curricula on many downstream tasks. We believe if the broader community had an easy to use benchmark they could focus on studying the broad effects of curricula. 