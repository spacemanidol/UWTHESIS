\chapter{Future Work}
\label{chap:future}
We seek to continue our exploration of CL methods and methods to make training and using models more efficient in the future. Since we began our work there has been much exciting work on finding sub-networks in NNs that preserve the accuracy of the original model \cite{Frankle2019TheLT}. Other methods have explored the pruning of large models for smaller equally accurate models \cite{Han2016DeepCC} \cite{Yu2017ScalpelCD} \cite{Wynter2020OptimalSE} which we would like to expand on, focusing on how these pruning methods perform with transfer learning. Additionally, inspired by the layered nature of Transformer-based models, we would explore jointly how progressive methods may look like for LMs. One such approach would be to increase the number of transformer encoders while increasing data difficulty, similar to what has been done with GANs \cite{Karras2017ProgressiveGO}. Another method may model longer context windows progressively starting in short sentences and reaching entire documents. Finally, in the future, we seek to make a benchmarking system that can allow authors to explore the effect of various curricula on many downstream tasks. If the broader community had an easy to use a benchmark, they could focus on studying the overall impact of curricula.  The goal would be to provide a framework similar to GLUE and JIANT, which provide a set of architectures and tasks which would allow researchers to focus only on sampling methods.