\chapter{Future Work}
\label{chap:future}
We seek to continue our exploration of CL methods and methods to make training and using models more efficient. Since we began our work there has been much exciting work on finding sub-networks in NNs that preserve the accuracy of the original model \cite{Frankle2019TheLT}. Other methods have explored the pruning of large models for smaller equally accurate models \cite{Han2016DeepCC} \cite{Yu2017ScalpelCD} \cite{Wynter2020OptimalSE} which we would like to expand on, focusing on how these pruning methods perform with transfer learning. While these method are not directly computationally more efficient to train they make reuse of some original large model (which can be trained once) and allow for more efficient model deployment. Additionally, inspired by the layered nature of Transformer-based models, we would explore jointly how progressive methods may look like for LMs. One such approach would be to increase the number of transformer encoders while increasing data difficulty, similar to what has been done with GANs \cite{Karras2017ProgressiveGO}. Another method may model longer context windows progressively starting with short sentences and scaling to entire documents. Finally, we seek to make a benchmarking system that can allow researchers to explore the effect of various curricula on many downstream tasks. This benchmark would include tasks from NLP, CV, and beyond and the focus for researchers would be on the training regime. The goal would be to provide a framework similar to GLUE and JIANT, which provide a set of architectures and tasks which would allow researchers to focus only on sampling methods. If the broader community had an easy to use a benchmark, they could focus on studying the just curriculum methods. 