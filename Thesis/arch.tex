\section{Architecture}
\label{chap:prior:sec:arch}
In this section, we first briefly review of neural networks in NLP in Section \ref{chap:prior:sec:arch:nn} so as to obtain insight into how system architecture can effect downstream tasks. Then, in Section \ref{chap:prior:sec:arch:lstm} we introduce Long Short Term Memory(LSTM) \cite{Hochreiter1997LongSM} and describe how it works. In Section \ref{chap:prior:sec:arch:transformer} we introduce the Transformer \cite{Vaswani2017AttentionIA} and briefly discuss how it differs from LSTM.
\subsection{Neural Network in NLP}
\label{chap:prior:sec:arch:nn}
In the last few decades, neural networks have provided language researchers with an effective method of building models that represent the complex phenomena associated with language. In NLP, Recurrent Neural Networks (RNN) are popular building blocks because they can retrain contextual information from previous states. While useful, RNNs are limited in their ability to represent a full sequence by their short-term memory. When presented with long sequences of input, the amount of information needed to represent the total input and how individual components relate grows. Since this state is limited in size as sequences get more prolonged, less context about particular parts of the sequence is kept in the hidden state. The slow loss of prior input's information can be incredibly impactful for for nuances of language like agreement on verges and coreference, which can be many sentences apart. Besides issues in representing inputs with large context windows, RNNs suffer from what is known as the vanishing gradient problem \cite{Hochreiter1998TheVG}. The vanishing gradient problem focuses on how, when training RNNs, we must update the weights of previous hidden states, but as we do so, their gradients get smaller and smaller. Eventually, the gradient on earlier weights is so small that it practically vanishes, and older weights are not updated. To address this shortcoming and improve modeling long term dependencies architectures like LSTM and the Transformer were introduced. 
\subsection{LSTM}
\label{chap:prior:sec:arch:lstm}
The LSTM was proposed by Hochreiter et al.,  1997 \cite{Hochreiter1997LongSM} as a neural method that was better suited to avoid vanishing gradients. LSTMs have a gating mechanism that regulates the flow of information from previous states into the current state. What makes the LSTM unique is that it has forget gates that are trainable parameters which choose what information from earlier parts of the input is forgotten. Using these gates, LSTMs can retain relevant information even with large input sizes. Since LSTMs excel at data where their long term dependencies, they have revolutionized the world of time-series prediction \cite{Sagheer2019TimeSF}, speech recognition \cite{Han2017ESEES}, and neural machine translation (NMT) \cite{Jean2015OnUV} to name a few. 
\subsection{Transformer}
\label{chap:prior:sec:arch:transformer}
The Transformer is a neural system first proposed by Vaswani et al., 2017  \cite{Vaswani2017AttentionIA} as a system for NMT. The transformer block introduced a way to move from RNNs to an architecture that focuses on the interaction between units in the input data. Attention is a method that selects what portions of the input are related to each other. Instead of keeping a global state like an RNN, Transformers focus on distilling what input items are attended to by other parts of the data. Attention is a learned weight mechanism that represents how much one item in a list is related to others. In NLP, this method has been extremely successful as it allows a model to learn the relative salience of items with regard to every other item in the input. In its initial formulation, the implementation had an encoder and a decoder, consisting of 6 stacked transformer layers. All encoders are identical (but do not share weights) and have two sub-layers: self-attention and a feed-forward network. All decoders are identical (and do not share weights) and consist of 3 sub-layers: self-attention, encoder-decoder attention, and a feed-forward layer. \\ 
Vaswani et al., 2017 use multi-head attention to produce a linear projection used to attend to different parts of the input. Since this method does not rely on recurrence or convolution, a positional encoding is introduced to allow the Transformer to understand where in a sentence a word is. For more details about the mechanics of the Transformer, we recommend reading The Illustrated Transformer\footnote{https://jalammar.github.io/illustrated-transformer/}. In the original implementation,  stacked layers of transformers train translation systems for English to French and English to German translation, which results in new SOTA models with $\frac{1}{4}$ the training time. \\
The Transformer's ability to model long-term dependencies well and efficiently has made it a natural fit for most NLP tasks. As we will cover in succeeding sections, researchers have used this method to produce robust and usable language representations.