\section{Language Modeling}
\label{chap:prior:sec:lm}
In Section \ref{chap:prior:sec:lm:overview}, we briefly review what language modeling is, various methodologies, and why language modeling is so useful for NLP. Then, in Section \ref{chap:prior:sec:lm:elmo} introduce ELMo followed by BERT in Section \ref{chap:prior:sec:lm:bert}. Then, in Section \ref{chap:prior:sec:lm:otherlm}, we discuss some other major LMs and how the broader NLP community is using them. Finally, in Section \ref{chap:prior:sec:lm:effects}, we discuss some of the effects of training and using large LMs to ground our research's motivation.
\subsection{What is Language Modeling}
\label{chap:prior:sec:lm:overview}
Language modeling is a way to assign a probability distribution over some textual representation. In other words, if the task is to model $n$-grams, the probability of a current input is the probability of a token $w_i$ given the previous $i$ tokens. This is commonly factorized as \fullref{equation:langmodel}:
\begin{equation}
    P(w_{1},\ldots ,w_{m})=\prod _{i=1}^{m}P(w_{i}\mid w_{1},\ldots ,w_{i-1})\approx \prod _{i=1}^{m}P(w_{i}\mid w_{i-(n-1)},\ldots ,w_{i-1})
\label{equation:langmodel}
\end{equation} Language models can be useful methods to represent natural language because they allow models to differentiate meanings of sentences based on context. In other words a model is able to understand that the word `fly' can mean different things in the sentences: `You look fly', `Lets fly away!', `That is a fly'. \\
While language modeling is by no means a new concept, it was not until the introduction of Neural Network based LM that these representations were able to serve as general understanding frameworks. Before these Neural Network Language Models (NNLM), most language modeling usually focused on modeling some form of an N-gram where the probability of a word only depends on the previous N-words. Large Neural-Network-based LMs are the first step in an NLP application as a way of turning some form of textual input into a representation in a vector space. \\
Language models are created using many training objectives, but general models tend to be either auto-encoding (AE), auto-regressive (AR) or some combination of the two. AR models like ELMo \cite{Peters2018DeepCW} or GPT-2 \cite{Radford2019LanguageMA} learn a LR by predicting the next token in a sequence. AE models like BERT \cite{Devlin2019BERTPO} and ELECTRA \cite{Clark2020ELECTRAPT} learn a LR by reconstructing some portion of a sequence.
\subsection{ELMo}
\label{chap:prior:sec:lm:elmo}
ELMo is an AR LM that was introduced by Peters et al., 2018 \cite{Peters2018DeepCW} and, in many ways, became the first contextual word representation that saw widespread usage. The name ELMo represents Embeddings from Language Models and refers to how language modeling can be used to train contextual word embeddings. ELMo is an auto-regressive model built off the success of GloVe \cite{Pennington2014GloveGV}, and Word2Vec \cite{Mikolov2013EfficientEO} by seeking to be the first stage of textual processing for a variety of NLP tasks. ELMo consists of a character-level convolutional neural network (CNN) followed by two layers of bidirectional LSTMs. The CNN is used to convert words from a text string into raw word vectors, which are then passed to the BiLSTMs to model the whole input. Using a character level CNN, ELMo can capture the inner morphological structure of words, e.g., words like beauty and beautiful are similar when character level convolutions are used.\\
Each layer receives a forward pass and backward pass over the textual input, which allows the model to read the sentence left-to-right and right-to-left and form representations that understand the whole context of a sentence. The forward pass of the text (reading left to right) allows the model to build context for a word given previous words, while the backward pass (reading right to left) allows the model to build context from the end of the input to the word being modeled. The backward and forward pass are concatenated together. The output of the first biLSTM is passed into the second layer, and then the final ELMo representation is the weighted sum of the raw word vectors and the two intermediate word vectors (outputs of each biLSTM).\\ 
ELMo was trained using the Billion Word Corpus \cite{Chelba2014OneBW} and using the unprocessed input as the target for ELMo's language modeling task. The model is trained for ten epochs (complete passes on the corpus), which takes approximately three weeks using three 1080ti GPUs. On average, the authors find that adding ELMo as a text representation layer provides 20\% improvement across a diverse set of NLP tasks.
\subsection{BERT}
\label{Bchap:prior:sec:lm:bert}
Building on the success of ELMo, leveraging the transformer architecture \cite{Vaswani2017AttentionIA}, and taking the learnings from other contextual word embeddings \cite{Howard2018UniversalLM} \cite{Radford2018ImprovingLU} Devlin et al., 2018 introduced BERT, which stands for Bidirectional Encoder Representations from Transformers. BERT is an AE LM that uses modified stacked Transformer encoders (12 layers for a small model and 24 for large) to build a contextual language representation. Instead of using character-level convolutions or fixed word vectors as a starting point, BERT leverages a piecewise tokenization \cite{Wu2016GooglesNM}, which sets a vocabulary size of 30,000.  \\
Just like other language models before it, BERT trains using unsupervised pre-training on a large text corpus. Unlike previous models, BERT introduces two new training objectives as a way to steer the model: Masked Language Modeling (MLM) and next sentence prediction (NSP). \\
MLM reformulates language understanding as a cloze task \cite{Taylor1953ClozePA}, where the model's goal is to predict what a hidden word in a sentence may be. To train using MLM BERT introduces a new token $[MASK]$ to represent the hidden word. 15\% of each the corpus tokens are selected to be replaced of which 80\% (12\% of the corpus) are replaced with $[MASK]$, 10\%( 1.5\% of the corpus) are replaced with a random token, and the remaining 10\% are left alone. When the model finds a $[MASK]$ token, it predicts what the word should be. NSP is a training method inspired by QA systems, which tend to have two sets of sentences to reason on: a query and a context passage. In NSP, the model is fed text, which combines two sentences, A and B, with the unique separation token [SEP]. In 50\% of the NSP samples, sentence B directly follows A while in the remaining 50\% A, and B are selected at random. The model has a binary training goal if the sentences are next to each other in the original text.\\
When the BERT architecture and training regime is trained on the Toronto Book Corpus \cite{Zhu_2015_ICCV} (800m words) + English Wikipedia (2.5 billion words), the authors can create a generalizable contextual word embedding, which since the models release has fine-tuned on countless transfer tasks to produce new SOTA models.
\subsection{Beyond BERT}
\label{chap:prior:sec:lm:otherlm}
Besides BERT and ELMo, there has been considerable research into additional language models. RoBERTa \cite{Liu2019RoBERTaAR} improves on BERT by training on a larger corpus for a longer time. XLNET \cite{Yang2019XLNetGA} combines AE and AR while avoiding some of the pitfalls of each method by modifying AR to maximize the expected log-likelihood of a sequence concerning all permutations of factorization order. XLNET also removes the notion of a $[MASK]$ token to avoid training the model with a token that never occurs in text and implements the whole architecture using the Transformer-XL \cite{Dai2019TransformerXLAL}. ALBERT \cite{Lan2019ALBERTAL} explores the role of size in LM, finding that parameter weights can be shared across layers meaning they can have 18 times fewer parameters and train 1.7x faster than regular BERT all while producing similar language representation to BERT. DistilBERT \cite{Sanh2019DistilBERTAD} produces a smaller LM using knowledge distillation resulting in a similar performance to BERT with a 40\% smaller model. GPT \cite{Radford2018ImprovingLU}, GPT-2 \cite{Radford2019LanguageMA}, and GPT-3 \cite{Brown2020LanguageMA}  build an AR LM more suited toward language generation by using progressively larger models and a modified transformer decoder architecture. ELECTRA \cite{Clark2020ELECTRAPT} produces a model with comparable performance to BERT with substantially shorter training by having the model predict all tokens in a sentence instead of the $[MASK]$ token and by corrupting the input using a Generator similar to that of a GAN. Beyond these few models we mention, countless other optimizations and applications of these large scale NNLM. 
\subsection{Language Model's Impact}
\label{chap:prior:sec:lm:effects}
In studying the performance of the rapidly growing NNLM, researchers have found that larger models are more sample efficient and reach a higher level of performance with fewer steps \cite{Kaplan2020ScalingLF}. Kaplan et al., 2020 find that the dataset size, model size, and compute used for training all have a power-law relationship with performance as long as the factors grow proportionally. The authors estimate that the best model would have about a trillion parameters, trained on a trillion word corpus using over 100 petaflops.\\
While there is no debate on the positive impact these large LMs have had on NLP, the broader research community has begun discussing the broader effects of these continually growing language models. A decade ago, most NLP research could be developed and trained on commodity laptops or servers. Competitive research usually requires multiple instances of specialized hardware like GPUs and TPUs \cite{Strubell2019EnergyAP}. Strubell et al., 2019 broadly studies the energy implications of training these NNLM and estimates that a single training run of a model like GPT-2 can cost upward of \$40,000, the architecture search and hyperparameter tuning can be upwards of \$3,000,000, and the C$0_2$ released by training one of these models can be similar to the C$0_2$ released in the entire life-cycle of a car. Zhou et al., 2020 \cite{Zhou2020HULKAE} introduce HULK to encourage researchers to think about efficiency in every stage of model creation. Looking at the impact of large language models, researchers can infer that some of the most interesting research in NLP will be focused on how to scale model size while balancing the increased cost in doing so. \\
