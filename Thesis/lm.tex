\section{Language Modeling}
In this section, in section 2.2.1 we briefly review of what language modeling is, various methodologies, and why language modeling is so useful for NLP. and then explore why its relevant for NLP.  Then, in section 2.2.2 introduce ELMO followed by BERT in 2.2.3. Then, in 2.2.4 we discuss some other major LMs and how the broader NLP community is using them. Finally in 2.2.5 we discuss some of the effects of large LMs to ground the motivation of our research.
\subsection{What is Language Modeling}
Language modeling is a way to assign a probability distribution over some textual representation. In other words language modeling is an application of conditional probably, E.G. $P(w_{1},\ldots ,w_{m})P(w_{1},\ldots ,w_{m}) $ where $w_i$ is a word in a sentence. Language models can be useful methods to represent natural language because they allow models to differentiate meanings of sentences based on context. In other words a model is able to understand that the word 'fly' can mean different things in the sentences: "You look fly", "Lets fly away!", "That is a fly". \\
While language modeling is by no means a new concept it was not until the introduction of Neural Network based LM that these representations were able to serve as general understanding frameworks. Prior to these Neural Network Language Models(NNLM) language modeling usually focused on modeling some some form of an n-gram where the probability of a word only depends on the previous n words. Large Neural Network based LMs are usually used as the first step in a NLP application as a way of turning some form of textual input into a representation in a vector space.
Language models can be created using many training objectives but in general models tend to be either auto encoding or auto regressive. Auto Encoding models seek to learn an efficient representation for language through training the signal to ignore data. Auto regressive models learn language by building language into conditional probability with the logic that if a model learns to predict previous or next words it must be able to predict language given context.
\subsection{ELMO}
ELMO was introduced by Peters et' al 18 \cite{Peters2018DeepCW} and in many ways became the first contextual word representation that saw widespread usage. The name ELMO represents Embeddings from Language MOdels and is a reference to how language modeling can be used to train contextual word embeddings. ELMO is an auto-regressive model built off the success of GloVe \cite{Pennington2014GloveGV} and Word2Vec \cite{Mikolov2013EfficientEO} by seeking to be the first stage of textual processing for a variety of NLP tasks. ELMO consists of a character-level convolutional neural network(CNN) followed by two layers of bidirectional LSTMs. The CNN is used to convert words from a text string into raw word vectors which are then passed to the BiLSTMs to model the whole input. By using a character level CNN ELMO is able to capture the inner morphological structure of words, e.g. words like beauty and beautiful are similar when character level convolutions are used.\\
Each layer receives a forward pass and backward pass over the textual input which allows the model to read the sentence left to right and right to left and form representation that understand the whole context of a sentence. The forward pass of the text(reading left to right) allows the model to build context for a word given previous words while the backward pass(reading right to left) allows the model to build context from the end of the input to the word being modeled. The backward and forward pass are concatenated together. The output of the first biLSTM is passed into the second layer and then the final ELMO representation is the weighted sum of the raw word vectors and the 2 intermediate word vectors(outputs of each biLSTM).\\ 
The ELMO model was trained by using the Billion Word Corpus \cite{Chelba2014OneBW} and using the unprocessed input as the target for ELMO's language modeling task. The model is trained for 10 epochs(complete passes on the corpus) which takes approximately 3 weeks using 3 1080ti GPUs. On average the authors find that adding ELMO as a text representation layer provides 20\% improvement across a diverse set of NLP tasks.
\subsection{BERT}
Building on the success of ELMO, leveraging the transformer architecture \cite{Vaswani2017AttentionIA}, and taking the leanings from other contextual word embeddings \cite{Howard2018UniversalLM} \cite{Radford2018ImprovingLU} Devlin et. al 18' introduced BERT which stands for Bidirectional Encoder Representations from Transformers. BERT seeks to model language by being an auto encoder that uses modified stacked transformer encoders (12 layers for small model and 24 for large) to build a contextual language representation. Instead of using character level convolutions or fixed word vectors as a starting point BERT leverages a piecewise tokenization \cite{Wu2016GooglesNM} which sets a vocabulary size of  30,000 which allows BERT to exploit the natural morphology in language.  \\
Just like other language models before it, BERT uses unsupervised pre training on a large text corpora to train the model. Unlike previous models BERT introduces two new training objectives as a way to steer the model: Masked Language Modeling(MLM) and next sentence prediction(NSP). 
MLM reformulates the language understanding as a cloze task \cite{Taylor1953ClozePA} where the models goal is to predict what a hidden word in a sentence may be. To train using MLM BERT introduces a new token '<MASK>' to represent the hidden word. 15\% of each the corpus tokens are selected to be replaced of which 80\%(12\% of the corpus) are replaced with '<MASK>', 10\%(1.5\% of the corpus) are replaced with a random token, and the remaining 10\% are left alone. When the model finds a '<MASK>' token it predicts what the word should be. NSP is a training method inspired by QA systems which tend to have 2 sets of sentences. In NSP the model is fed text which is the combination of two sentences A and B with the unique separation token '[SEP]'. In 50\% of the NSP samples sentence B directly follows A while in the remaining 50\% A and B are selected at random.\\
When the BERT architecture and training regime is trained on  book corpus(800m words) + English Wikipedia(2.5 billion words) the authors are able to create a highly efficient and generalizable contextual word embedding which at the time of the models release created a new SOTA for countless tasks. On GLUE BERT is able to 7\% over the previous SOTA. 
\subsection{Beyond BERT}
Besides BERT and ELMO there has been considerable research into additional language models. RoBERTa \cite{Liu2019RoBERTaAR} improves on BERT by training on a larger corpus for longer creating. XLNET \cite{Yang2019XLNetGA} combines auto encoding and auto regression while avoiding some of the pitfalls of each method by modifying auto regression by maximizing the expected log likelihood of a sequence with regard to all permutations of factorization order. XLNET also removes the notion of a mask token to avoid training with a token that never occurs in text and uses the Transformer-XL. ALBERT \cite{Lan2019ALBERTAL} explores the role of size in LM finding that parameter weights can be shared across layers meaning they are able to have 18x less parameters which trains 1.7x faster than regular BERT all while producing similar language representation to BERT. ALBERT also introduces DistilBERT \cite{Sanh2019DistilBERTAD} produces a smaller model using knowledge distillation resulting in similar performance with 40\% smaller model. The GPT family of models: GPT \cite{Radford2018ImprovingLU}, GPT-2 \cite{Radford2019LanguageMA}, GPT-3 \cite{Brown2020LanguageMA} build an auto regressive LM more suited toward generation using progressively larger models and a modified transformer decoder architecture. ELECTRA \cite{Clark2020ELECTRAPT} produces a model with comparable performance to BERT with substantially shorter training by having the model predict all tokens in a sentence instead of the '<MASK>' token. Beyond these few models we mention there have been countless other optimizations and applications of these large scale NNLM. 
\subsection{Language Model's Impact}
In studying the performance of the rapidly growing NNLM researchers have found that larger models are more sample efficient and reach a higher level of performance with fewer steps \cite{Kaplan2020ScalingLF}. Kaplan et al. 20' find that the dataset size, model size, and compute used for training all have a power law relationship with performance as long as one of the factors isn't bottlenecks. Kaplan et al. 20' estimate that the best model would have about a trillion parameters, trained on a trillion word corpus using over 100 peta-flops.
While there is no debate on the positive impact that these large LMs have had on NLP the broader research community has begun discussion about the broader effects of these continually growing language models. A decade ago most NLP research could be developed and trained on commodity laptops or servers. Now competitive research usually requires multiple instances of specialized hardware like GPUs and TPUs \cite{Strubell2019EnergyAP}. Strubel et al. 19' broadly studies the energy implications of training these NNLMs estimating that a single training run of a model like GPT-2 can cost upward of \$40,000 and the architecture search and hyperprarameter tuning can be upwards of \$3,000,000 and the C02 released by training one of these models can be similar to the C02 released in the life cycle of a car. To encourage researcher to think about efficiency in every stage of a model Zhou et al. 20' \cite{Zhou2020HULKAE} introduce a new benchmark which compares the energy efficiency(cost and time) of various approaches. Looking at the impact of large language models it can be inferred that some of the most interesting research in NLP will be focuses on how to continue to gain improvements gained by size while balancing the increased cost in training these larger models. \\
