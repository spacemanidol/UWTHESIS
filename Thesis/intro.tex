\chapter {Introduction}
\label{chap:intro:sec:intro}
The ability to understand language has long been a fascination for computer scientists worldwide. Early systems like ELIZA \cite{Weizenbaum1966ELIZAA} were designed to interact with humans through verbal communication. Despite being rudimentary and simple, these systems quickly caught the world's attention and inspired countless researchers to hopefully one day understand and emulate language.  In the decades since, researchers have grown to understand fundamental concepts about language structure and how best to model it. Creating systems that can understand language and preserve its meaning in downstream tasks has long focused on the NLP community. Leveraging deep neural networks (DNN), researchers have created language representations that represent and understand many of the nuances of human language. These methods have revolutionized nearly every system that works with human language at a pace that seems to be getting faster and faster. Despite their impressive performance, these models are difficult and expensive to create (and recreate) and deploy. We will explore whether the use of CL can improve model training and model accuracy.\\
In the remainder of this chapter, we first briefly describe the problem we want to tackle and how we will tackle these problems in \fullref{chap:intro:sec:overview}. Then, we give a brief overview of Curriculum Learning and Language Representations in \fullref{chap:intro:sec:cl} and \fullref{chap:intro:sec:lm} respectively. We briefly discuss this dissertation's goals and challenges in sections \fullref{chap:intro:sec:goals}. We then summarize the contributions of this dissertation in section \fullref{chap:intro:sec:contributions}. We then provide a brief outline of the Dissertation structure in section \fullref{chap:intro:sec:structure} before presenting our statement of originality in section \fullref{chap:intro:sec:statement}.
\section{Overview}
\label{chap:intro:sec:overview}
This dissertation addresses the topic of the effect of Curriculum Learning methods on Language Models. The term Curriculum Learning (CL) refers to a training procedure where the training data for a machine-learned method is constructed with a deliberately assembled order with the goal of steering training to a more optimal solution sooner. Usually, CL methods form a heuristic to assign a difficulty score to each example and then gradually increase the training data's difficulty. The term Language Model(LM) refers to determining the probability for a given text window. Language modeling can be used to train a Neural Network (NN) to understand language and represent language for downstream Natural Language Processing (NLP) tasks like Question Answering (QA) and Information Retrieval (IR). In this dissertation, we will explore the effect CL methods have on LMs.
\section{Curriculum Learning}
\label{chap:intro:sec:cl}
Curriculums have long been part of the way humans learn. In our schooling, instructors assemble information and instruction in a way that allows students to learn foundations and more straightforward concepts first before they move onto difficult samples. In traditional machine learning, for each step of the network training, a portion of the training data is sampled randomly. Unlike students in school, networks must learn to understand difficult examples while learning to understand simple examples. There are many formalizations and implementations for curriculum-like methodologies in machine learning, but for this dissertation, we will focus on the formalization in neural networks by Bengio et al., 2009 \cite{Bengio2009CurriculumL}. In their formalization, curriculum learning is a method for altering the training data distribution to allow the network to more straightforward concepts before difficult concepts.
\section{Representing Language}
\label{chap:intro:sec:lm}
In the frame of language understanding, researchers have long focused on representing all the information that text represents in a way that can be used by other systems. Researchers have explored human-curated representations like WordNet \cite{Miller1992WordNetAL}  and explore statistical methods to represent corpora \cite{Leacock1993TowardsBC}. More recently, statistical methods have tried to produce vector space models where words are represented by a set of vectors in an N-dimensional space. In the early 2010s, methods like Word2Vec \cite{Mikolov2013EfficientEO}, and GloVe \cite{Pennington2014GloveGV} were introduced, and the broader NLP community found great leverage in vector-based representations of language. Since these word representations mapped each word to a lower-dimensional numeric representation, systems could now take advantage of the closeness of words via methods like cosine similarity. Moreover, these vector representations of words proved to be a powerful method to represent language for many downstream tasks like: Information Retrieval \cite{Roy2018UsingWE}, Question Answering \cite{Othman2017AWE}, Sentiment Analysis \cite{Zhou2016ECNUAS}, Machine Translation \cite{Zou2013BilingualWE}, etc. \\
These fixed word vectors drove incredible improvement and research across many fields, but since representations focused on word-level vector values, systems could not represent words in the context they occur in. Moreover, since these context-independent word representations cannot differentiate between usages at best, each word representation is the average of all of its different uses. Context is particularly essential for words with multiple meanings but the same surface form like \emph{fly}. The representation is the same if the context was dealing with an insect (There is a \emph{fly} in my soup!), an action (I want to \emph{fly} away.), or a description of personal style(You sure look \emph{fly}!). To expand word representation out of discrete words, researchers explore methods like having multiple word vectors for each meaning of a word \cite{Hu2016DifferentCL}, sentence vectors \cite{Kiros2015SkipThoughtV}, paragraph vectors \cite{Le2014DistributedRO}, Gaussian mixtures \cite{Athiwaratkun2017MultimodalWD}, and sub-word vectors \cite{Bojanowski2017EnrichingWV}. These methods seek to improve models' ability to capture expressive semantic information and perform better in polysemy situations. While these techniques provided incremental improvements, they all continued to lack a robust way to represent sentences outside of the original training corpus, and minor changes in sentence structure could have a large effect on the vector representation. \\ 
In explorations on improving word representations, researchers used LM to learn contextual word representations. Language modeling is a well-defined NLP problem that aims to model a statistical distribution of words in a sentence to look for some prediction about the next word. Using LM systems with Sesame Street-inspired names like ELMo \cite{Peters2019KnowledgeEC}, BERT \cite{Devlin2019BERTPO}, ERNIE \cite{Sun2019ERNIEER} were able to produce what could seem like a never-ending stream of new State-of-the-art (SOTA) models that excelled at most NLP problems. The success of these models spun a seemingly continuous wave of new techniques building of BERT like KnowBERT \cite{Peters2019KnowledgeEC}, RoBERTa \cite{Liu2019RoBERTaAR}, AlBERT \cite{Lan2019ALBERTAL}, ELECTRA \cite{Clark2020ELECTRAPT}, and BART \cite{Lewis2019BARTDS}.  \\
While these deep learning-based LMs have shown to be excellent methods to enable language understanding in many tasks, the ability to train these models is becoming increasingly computationally expensive \cite{Strubell2019EnergyAP}. The research in new LMs has been dominated by large research labs with multi-million dollar budgets and vast propriety corpora because it has been shown that model performance is closely tied to the size of training data, model size, and compute used to train \cite{Kaplan2020ScalingLF}. Since this correlation has proven fruitful, the focus on the community has not been on the application of alternative learning methodologies to the established and successful architecture.
\section {Goals and Challenges}
\label{chap:intro:sec:goals}
The goal of this dissertation is to explore how CL training methods affect LMs. First, we explore strategies for evaluating the difficulty of a sentence or a line of text. Next, using difficulty scores, we then explore how structured training regimes affect model performance. This dissertation's final goal is to evaluate the effect that various CL methods have on language modeling, which we would seek to apply to the overall architecture and size of LMs. \\
This dissertation's main challenge is to scope the language model experimentation to allow us to perform the broadest and most representative experiments. Doing an exhaustive search of effects on many language models will prove to be computationally prohibitive, and thus scope will be limited. Ensuring our work is representative of variations in model size, and dataset size will also be challenging. 
\section{Contributions}
\label{chap:intro:sec:contributions}
Our contributions are threefold: LM performance as measured by perplexity on the training corpus is not a predictive measure of transfer learning performance, CL method can produce better contextual word representations on smaller corpora, but as corpus size scales, CL methods make worse models, and random curricula can be as effective curricula as structural or linguistically motivated curricula.
\section{Structure of This Dissertation}
\label{chap:intro:sec:structure}
In \fullref{chap:prior},we present the background material in language modeling, curriculum learning, popular Neural Network architectures, and methods for evaluating Language Models. We give a detailed and technical introduction to Curriculum Learning and discuss some recent applications in the NLP domain for CL. Concerning LM, we introduce the concept of language modeling, describe various methodological advances and improvements discovered over time, and contrast multiple implementations. We will also provide a brief introduction to large language models' effects and why focusing on model training efficiency is essential. Finally, we introduce some of the most popular methods for evaluating language modeling along with the datasets and frameworks we will use for our experiment. \\
In \fullref{chap:method}, we provide a detailed explanation of our experimental setup. We propose two curriculum learning strategies building on prior work and explain how we implemented these strategies and how we evaluate the effectiveness. 
In \fullref{chap:results}, we discuss the results of our experiments along with a comprehensive analysis of the impact our various methodologies had on model performance.
In Chapter \fullref{chap:conclusion}, we conclude this dissertation.
In Chapter \fullref{chap:future}, we discuss future work we would like to research.
\section{Statement of Originality}
\label{chap:intro:sec:statement}
I declare that this dissertation was composed by myself and that the work it presents is my own, except where otherwise stated.