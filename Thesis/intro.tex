\chapter {Introduction}
\label{chap:intro:sec:intro}
The ability to understand language has long been a fascination for computer scientists worldwide. Early systems like ELIZA \cite{Weizenbaum1966ELIZAA} were designed to interact with humans through verbal communication. Despite being rudimentary and simple, these systems quickly caught the world's attention and inspired countless researchers to hopefully one day understand and emulate language.  In the decades since, researchers have grown to understand fundamental concepts about language structure and how best to model it. Creating systems that can understand language and preserve its meaning in downstream tasks has long focused on the Natural Language Processing (NLP) community. Leveraging deep neural networks (DNN), researchers have created language representations (LR) that seek to represent many of the nuances of human language. These methods have revolutionized nearly every system that works with human language at a pace that seems to be getting faster and faster. Despite their impressive performance, these models are difficult and expensive to create (and recreate) and deploy. We will explore whether the use of curriculum learning (CL) can improve model training and model accuracy.\\
In the remainder of this chapter, we first briefly describe the problem we want to tackle and how we will tackle these problems in Section \ref{chap:intro:sec:overview}. Then, we give a brief overview of CL and LR in Section \ref{chap:intro:sec:cl} and Section \ref{chap:intro:sec:lm} respectively. We briefly discuss this dissertation's goals and challenges in Section \ref{chap:intro:sec:goals}. We then summarize the contributions of this dissertation in Section \ref{chap:intro:sec:contributions}. We then provide a brief outline of the Dissertation structure in Section \ref{chap:intro:sec:structure} before presenting our statement of originality in Section \ref{chap:intro:sec:statement}.
\section{Overview}
\label{chap:intro:sec:overview}
This dissertation addresses the topic of the effect of CL methods on Language Models (LM). The term CL refers to a training procedure where the training data for a machine-learned method is constructed with a deliberately assembled order with the goal of steering training to a more optimal solution sooner. Usually, CL methods form a heuristic to assign a difficulty score to each example and then gradually increase the training data's difficulty. The term LM refers to determining the probability for a given text window. Language modeling can be used to train a Neural Network (NN) to understand language and represent language for downstream Natural Language Processing (NLP) tasks like Question Answering (QA) and Information Retrieval (IR). In this dissertation, we will explore the effect CL methods have on LMs.
\section{Curriculum Learning}
\label{chap:intro:sec:cl}
Curricula have long been part of the way humans learn. In our schooling, instructors assemble information and instruction in a way that allows students to learn foundations and more straightforward concepts first before they move onto difficult samples. In traditional machine learning, for each step of the network training, a portion of the training data is sampled randomly. Unlike students in school, networks must learn to understand difficult examples while learning to understand simple examples. There are many formalizations and implementations for curriculum-like methodologies in machine learning, but for this dissertation, we will focus on the formalization in neural networks by Bengio et al., 2009 \cite{Bengio2009CurriculumL}. In their formalization, curriculum learning is a method for altering the training data distribution to allow the network to more straightforward concepts before difficult concepts.
\section{Representing Language}
\label{chap:intro:sec:lm}
In the frame of language understanding, researchers have long focused on representing all the information that text represents in a way that can be used by other systems. Researchers have explored human-curated representations like WordNet \cite{Miller1992WordNetAL}  and statistical methods to represent corpora \cite{Leacock1993TowardsBC}. More recently, statistical methods have tried to produce vector space models where words are represented by a set of vectors in an N-dimensional space. In the early 2010s, methods like Word2Vec \cite{Mikolov2013EfficientEO}, and GloVe \cite{Pennington2014GloveGV} were introduced, and the broader NLP community found great leverage in vector-based representations of language. Since these word representations mapped each word to a lower-dimensional numeric representation, systems could now take advantage of the closeness of words via methods like cosine similarity. Moreover, these vector representations of words proved to be a powerful method to represent language for many downstream tasks like: Information Retrieval \cite{Roy2018UsingWE}, Question Answering \cite{Othman2017AWE}, Sentiment Analysis \cite{Zhou2016ECNUAS}, Machine Translation \cite{Zou2013BilingualWE}, etc. \\
These fixed word vectors drove incredible improvement and research across many fields, but since representations focused on word-level vector values, systems could not represent words in the context they occur in. Moreover, since these context-independent word representations cannot differentiate between usages at best, each word representation is the average of all of its different uses. Context is particularly essential for words with multiple meanings but the same surface form like \emph{fly}. The representation is the same if the context was dealing with an insect (There is a \emph{fly} in my soup!), an action (I want to \emph{fly} away.), or a description of personal style(You sure look \emph{fly}!). To expand word representation out of discrete words, researchers explore methods like having multiple word vectors for each meaning of a word \cite{Hu2016DifferentCL}, sentence vectors \cite{Kiros2015SkipThoughtV}, paragraph vectors \cite{Le2014DistributedRO}, Gaussian mixtures \cite{Athiwaratkun2017MultimodalWD}, and sub-word vectors \cite{Bojanowski2017EnrichingWV}. These methods seek to improve models' ability to capture expressive semantic information and perform better in polysemy situations. While these techniques provided incremental improvements, they all continued to lack a robust way to represent sentences outside of the original training corpus, and minor changes in sentence structure could have a large effect on the vector representation. \\ 
In explorations on improving word representations, researchers used LM to learn contextual word representations. Language modeling is a well-defined NLP problem that aims to model a statistical distribution of words in a sentence to look for some prediction about the next word. Large DNN with Sesame Street-inspired names \cite{Peters2019KnowledgeEC} \cite{Devlin2019BERTPO} \cite{Sun2019ERNIEER} have leveraged language modeling to produce LR that enable what could seem like a never-ending stream of new state of the art (SOTA) models on many NLP problems. The success of these models has caused what seems to be a flywheel of new LRs with new implementations like KnowBERT \cite{Peters2019KnowledgeEC}, RoBERTa \cite{Liu2019RoBERTaAR}, AlBERT \cite{Lan2019ALBERTAL}, ELECTRA \cite{Clark2020ELECTRAPT}, and BART \cite{Lewis2019BARTDS} producing gradually improving results on a variety of metrics. \\
While these deep learning-based LMs have shown to be excellent methods to enable language understanding in many tasks, the ability to train these models is becoming increasingly computationally expensive \cite{Strubell2019EnergyAP}. The research in new LMs has been dominated by large research labs with multi-million dollar budgets and vast propriety corpora because it has been shown that model performance is closely tied to the size of training data, model size, and compute used to train \cite{Kaplan2020ScalingLF}. Since this correlation has proven fruitful, the focus on the community has not been on the application of alternative learning methodologies to the established and successful architecture.
\section {Goals and Challenges}
\label{chap:intro:sec:goals}
The goal of this dissertation is to explore how CL training methods affect LMs. First, we explore strategies for evaluating the difficulty of a sentence or a line of text. Next, using difficulty scores, we then explore how structured training regimes affect model performance. Model performance will be evaluated by perplexity on the training data and performance on a transfer learning task. Model perplexity is an intrinsic evaluation which use to understand how good a language model is and model perplexity is $e^{loss}$ where $e$ is Euler's number and $loss$ is the model loss over the dataset. A transfer learning task when a model is applied to a novel task which is different to the original training task. The original training task is commonly called pre-training where as the transfer task is called fine-tuning. This dissertation's final goal is to evaluate the effect that various CL methods have on language modeling, which we would seek to apply to the overall architecture and size of LMs. \\
This dissertation's main challenge is to scope the language model experimentation to allow us to perform the broadest and most representative experiments. Doing an exhaustive search of effects on many language models will prove to be computationally prohibitive, and thus scope will be limited. Ensuring our work is representative of variations in model size, and dataset size will also be challenging. 
\section{Contributions and Findings}
\label{chap:intro:sec:contributions}
Our findings are as follows:
\begin{itemize}
  \item LM performance as measured by perplexity on the training corpus is not always a predictive measure of transfer learning performance.
  \item Using CL it is possible to produce better contextual word representations on small corpora when compared to non CL methods. 
  \item As corpus size scales CL methods produce worse contextual representations than non CL methods.
  \item A random curricula in which the structure of the training regime is random can be just as effective as linguistically motivated methods.
\end{itemize}
Our contributions are as follows:
\begin{itemize}
    \item We implement two curriculum learning methods specifically for language modeling and show that the methods can be useful for specific training constrains.
    \item We introduce a methodology for training language models using sampling with replacement and produce a thorough evaluation of effects.
    \item We produce a comparison of line and sentence based LM training methodologies showing that despite inherent differences in batch size there are no substantial improvement with either mechanism. Since sentence base methods are more computationally inefficient this our experiments provide support for continued utilization of line based training.
\end{itemize}
\section{Structure of This Dissertation}
\label{chap:intro:sec:structure}
In Chapter \ref{chap:prior},we present the background material in language modeling, curriculum learning, popular Neural Network architectures, and methods for evaluating Language Models. We give a detailed and technical introduction to Curriculum Learning and discuss some recent applications in the NLP domain for CL. Concerning LM, we introduce the concept of language modeling, describe various methodological advances and improvements discovered over time, and contrast multiple implementations. We will also provide a brief introduction to large language models' effects and why focusing on model training efficiency is essential. Finally, we introduce some of the most popular methods for evaluating language modeling along with the datasets and frameworks we will use for our experiment. \\
In Chapter \ref{chap:method}, we provide a detailed explanation of our experimental setup. We propose two curriculum learning strategies building on prior work and explain how we implemented these strategies and how we evaluate the effectiveness. 
In \ref{chap:results}, we discuss the results of our experiments along with a comprehensive analysis of the impact our various methodologies had on model performance.
In Chapter \ref{chap:conclusion}, we conclude this dissertation.
In Chapter \ref{chap:future}, we discuss future work we would like to research.
\section{Statement of Originality}
\label{chap:intro:sec:statement}
I declare that this dissertation was composed by myself and that the work it presents is my own, except where otherwise stated.