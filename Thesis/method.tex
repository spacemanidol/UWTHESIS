\chapter{Research Approach}
\label{chap:method}
To understand the effects that curriculum learning can have on LM, we will explore various curriculum methods on training, evaluation, and usage in downstream tasks. 
Our methodology is driven around three main research questions:
\begin{enumerate}
\item Does curriculum learning help a language model converge to a more optimal global minimum? 
\item Does the language representation learned via curriculum learning improve performance on downstream tasks when compared to non-curriculum methods?
\item  Do curriculum learning methods increase model convergence speed in both pre-training and downstream task fine-tuning?
\end{enumerate}
In Section \ref{chap:method:sec:structure}, we will first describe our experiment's structure as scoped to various implementations of curriculum learning. Next, in Section \ref{chap:method:sec:cldevelopment} we will describe and discuss our methods for curriculum development. Then in Section \ref{chap:method:sec:training} we will discuss how we will train and evaluate our LMs and how we will explore hyperparameter tuning.
\section{Experiment Structure}
\label{chap:method:sec:structure}
Our experimentation strategy is simple: train many models with a fixed structure and hyper-parameters using different curricula. Our implementation of this strategy is achieved by exploring two different curricula: corpus replacement style and competence based curriculum. Once a robust set of models are trained with each curriculum method, all models are evaluated on a held out portion of the training corpus and fine-tuned on the GLUE Benchmark. In the corpus replacement style (CRS) we create multiple distinct training data sets which are are used to train an unmodified LM. In the competence based curricula (CBC) we first assign a notion of difficulty to each sample in a dataset and then alter the batch sampling method within the language model to sample from a gradually difficult training data. Our CRS method provides few controls but is computationally cheap while the CBC provides ample control at the cost of computational overhead. \\
\subsection{Language Model}
To optimize how quickly we can train our systems, we will only explore the curriculum's effect on an established successful baseline, ELMo \cite{Smith2019ContextualWR}. We leverage the code\footnote{https://github.com/allenai/bilm-tf} used for the original ELMo experiments and use the same hyper-parameters reported in their paper. We make two changes to the original implementation: an ability to use a CBC and the introduction of padding token $<PAD>$. In the original implementation, the training loader will load a text file, shuffle all the lines, and then iterate through them. In our implementation of CBC, we load the full corpus, which we do not shuffle, then select a batch at random from the examples that meet our model's current competence. Our implementation changes data sampling to unconstrained random sampling without replacement to sampling with replacement. Since our implementation of competence curricula is based on sentence-level difficulty, it becomes possible that batches include sentences that are smaller than the defined context length that meets the original ELMo implementation (20 tokens). As a result, we have to introduce a padding token, $<PAD>$, which ensures that each sentence is at least the context window's size. To avoid having the model learn to predict the $<PAD>$ token, we introduce a masking function that sets the loss to 0 for any padding token. \\
It is worth noting that the introduction this token increases the computational cost to train a model as all padding tokens are essentially lost computation. To calculate the effect of this we counted the amount of padding tokens that are introduced in the wikitext-103 corpus by While we did not compute the cost of using this method for each of our curriculum methods we estimated the inefficiency introduce by this method by calculating the amount of padding tokens introduced. In the wikitext-103 corpus the training corpus has 101,425,658 tokens. When this is parsed into sentences and we sum the modulo of the context window(20) over all sentences we find we must introduce 42,547,653. If we do not split the corpus into sentences we still must introduce 12,204,311 tokens. This means that before even accounting for the cost of the curriculum implementation our model training requires about 12.03 or 41.95 percent more FLOPs. 
\subsection{Datasets}
For our training corpus, we leverage two well-established language modeling benchmarks of wikitext-2, and wikitext-103 \cite{Merity2016PointerSM}. These datasets collect verified good and featured articles from English Wikipedia and feature 2 million and 103 million tokens. The dataset comprises full articles with original punctuation, numbers, and case and the only processing is the replacement of all words which occur less than 3 times with a <UNK> token to represent unknown words . Each corpus has already been tokenized, processed, and split into train, validation, and evaluation components (80:10:10 based on splitting the source documents). While most other research on language modeling has been focusing on bigger and bigger data, our focus on smaller data allows us to experiment with more curriculum creation methods. We understand this will limit our model performance relative to current top methods as a smaller corpus limits the model performance \cite{Kaplan2020ScalingLF}. Moreover, these datasets were chosen because they are standard benchmarks for language modeling and sufficient size to train large scale, language models. We believe that the diversity of our two datasets (wikitext-103 is 50x larger than wikitext-2) will allow us to draw broad conclusions about curriculum learning independent of data size. More information about corpus size and diversity can be found in Equation \ref{table:corpussize}. Information about the corpus size used for other LM is included for comprehensiveness \cite{Chelba2014OneBW}.\\
\begin{table}[h!]
\begin{tabular}{|l|l|l|l|l|} \hline
\textbf{Corpus Name} & \textbf{vocabulary Size} & \textbf{Tokens} & \textbf{lines} & \textbf{sentences} \\ \hline
wikitext=2 & 33278 & 2507007 & 44836 & 131262 \\ \hline
wikitext-103 & 267735 & 103690236 & 1809468  & 5343947 \\ \hline
1B Word Benchmark & 793471 & 829250940 & N/A & N/A \\ \hline
\end{tabular}
\caption{Training Corpus details}
\label{table:corpussize}
\end{table} \\
As our competence methodology focuses on sentences, we make two versions of the wikitext datasets: sentence-level and line level. For our sentence level corpus, we leverage SPACY.IO's \cite{spacy2} \footnote{https://spacy.io/} sentence boundary detector to spit the original corpus to one delimited by sentences. The two different corpus splitting mechanisms mean that each of our competence based curricula (CBC) is run on four different datasets.
\subsection{Evaluation}
To evaluate our models, we will focus on three aspects of LM performance: performance on the trained task, performance on a downstream task, and the speed at which a model improves its performance on the trained task. The broad goal of our LMs is to represent a given corpus accurately and to do so we will evaluate model perplexity on the held-out portions of our datasets.
To measure the quality of our LMs downstream we will use the industry standard GLUE tasks.
\section{Curriculum Construction Methods}
\label{chap:method:sec:cldevelopment}
\subsection{Baseline}
Before we study the effect of curriculum learning on language modeling we first retrain the public ELMo model on our datasets and evaluate on GLUE. We also download the publicly released ELMo models and evaluate on GLUE. Our training of ELMo is done on for 10 epochs on the wikitext-2 and wikitext-103.
\subsection{Corpus Replacement Style}
Corpus Replacement Style is inspired by the the Bengio et al., 2009 \cite{Bengio2009CurriculumL} implementation of curriculum learning for LM. In their implementation, they train an AR LM with examples from a corpus with a context window of size 5. In the epoch the model trains on all 5 token spans which only contain the 5,000 most common tokens. For the second epoch, training mirrors the prior epoch but with a threshold of 10,000 most common words. This process continues until the model is training on a full unaltered corpus. \\
In early experiments we explored an approach modeled after the Bengio et al., 2009 method but as context size of ELMo is much larger performance suffered. Seeking to preserve the separation of model training and training data creation we created what we refer to as Corpus Replace Style (CRS) curriculum. In this implementation we simplify the corpus not by removing training spans but by replacing words less common than the threshold with an <UNK> token. Threshold selection was done by exploring various increment sizes and initial threshold size and our final most successful methods produced 6 unique datasets for input dataset. To match the training time of our baseline we train one epoch on each corpus and an additional 4 on the unaltered corpus. Details on corpus thresholds can be found in Table \ref{table:2}.
\begin{table}[h!]
\begin{center}
\begin{tabular}{|l|l|l|l|l|l|l|l|} \hline
\textbf{Corpus Name} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7-10}  \\ \hline
enwiki-2 & 5,000 & 10,000 & 15,000 & 20,000 & 25,000 & 30,000 & 33,278 \\ \hline
enwiki-103 & 25,000 & 50,000 & 75,000 & 100,000 & 150,000  & 250,000 & 267,735\\ \hline
\end{tabular}
\caption{Vocabulary size per epoch}
\label{table:2}
\end{center}
\end{table}
\subsection{Competence Method}
Following the work of \cite{Platanios2019CompetencebasedCL} introduced in Section \ref{chap:prior:sec:cl:cl} we will apply the notion of a CBC to LM training. CBC methods rely on the ability to assign a difficulty score to each sample in the training corpus and use this to only allow the model to train on samples that are easier than its current competence level. A model's competence score is defined by how far along in a training regime the model is. \\
Our training corpus, $X$ is a collection of sentences $S$, where each sentence $s_i$ is a sequence of words $s_i= w_o^i,w_1^i,...,w_n^i$ and each sentence is assigned a difficulty $\epsilon_{s_i}$. At each step in training model competence is represented by $\lambda_t$ where $\lambda_0$ represents the initial competence and $\lambda_{increment}$ represents how much model competence increases after each training batch. Prior to training, each sentence in the training data has been assigned a difficulty score from 0 to 1 represented by $\epsilon_{s_i}$. For each training batch, the model is only able to train on samples that have a difficulty where $\epsilon_{s_i} <= \lambda_t$. \\
To keep keep curriculum length for CBC and CRS equal we set our curriculum length to 6 epochs do a grid search on $\lambda_{increment}$ and $\lambda_0$ values. After our grid search we select the parameters that provide lowest training perplexity and set value $\lambda_0 = 0.1$  $\lambda_0 = 0.1$ for wikitext-2 and $\lambda_0 = 0.01$ $\lambda_{increment} = 0.00001$ for wikitext-103. The CBC based sampling algorithm is formalized in Algorithm \ref{algo:competence}. \\
\begin{algorithm}[H]
\label{algo:competence}
\SetAlgoLined
\KwResult{Model Trained with Competence Based Curricula}
Input: X, $\lambda_0$, $\lambda_{increment}$ \;
Compute difficulty, $\epsilon_{s_i}$ for $s_i \in X$\;
Compute Cumulative density of $\epsilon_{s_i}$\;
$\lambda_t = \lambda_0$\;
\For{training step t = 1,...,n}{
Sample batch $b$ from X such that $\epsilon_{s_i} < \lambda_t$\;
Train on batch $b$\;
$\lambda_{t+1} = \lambda_t + \lambda_{increment}$\;
}
\caption{Competence-based curriculum}
\end{algorithm}
To understand the effect of CBC we study 8 different curricula per dataset: 2 baseline and six different difficulty heuristics. The first baseline is a curricula method where we set $\lambda_0 = 1$, which means that our model can sample from the entire training dataset. This baseline aims to establish the effect that changing training with sample-without-replacement to sampling-with-replacement has on LM performance. The second baseline is a random curriculum where we sort the file randomly to create our sentence difficulty scores. The goal of this baseline is to establish the effect of any arbitrary curricula on LM training. The following six heuristics we explore are based on common NLP difficulty metrics, the original CBC paper, and some linguistically motivated difficulties. The heuristics are sentence length, unigram sentence probability, bigram sentence probability, trigram sentence probability, part of speech diversity, and sentence dependency complexity. For each methodology, for each $s_i$ in $X$, we compute a difficulty value for each sentence $\epsilon_{s_i}$ and then sort the dataset by this difficulty score. Using the sorted dataset we compute the cumulative density function (CDF) giving each sentence of the difficulty score $\epsilon_{s_i} \in [0,1]$. We will now describe each method.
\subsubsection{Sentence Length}
Formalized in Equation \ref{equation:sentencelen}, this curriculum is built on the idea that is a lot harder to model longer sentences, as longer sentences require better tracking of dependencies. We believe this method would be particularly effective in Transformer based models as it can steer the model into learning how to leverage its multi-headed attention with different sentence lengths. \begin{equation}
    \text{sentence-length-}\epsilon_{s_i} = length(s_i).
    \label{equation:sentencelen}
\end{equation} 
\subsubsection{Sentence Entropy}
Another part of language that can be difficult to model is words with a variety of frequency in the corpora. Models, if assumed to behave like humans, would find it difficult to understand the meaning of a word if they do not see it in a corpus nor have a diversity of usages to infer meaning. Since the statistical strength of training samples with rare words is low and the early model learned word embeddings are likely to have high variance it is likely that exposing a model early to rare words can result in badly estimated representations. To quantify this difficulty we propose producing a sentence entropy for each sentence with respect to its unigram, bigram, and trigram probabilities. These products can be thought of as an approximate naive language modeling as it assumes words are sampled independently. Note, we are not calculating the conditional probability of each word given the preceding N words but the probability of the N-gram given the text corpus. To produce a difficulty score $\epsilon_{s_i}$ we first calculate an $n$-gram probability for each unigram, bigram, and trigram in the training corpus. Then using this probability we calculate the $n$-gram difficulty of a input $s_i$  by computing the $\log$ product of each $n$-gram $\in s_i$ as show in Equation \ref{equation:gramprob}. $uc$, $bc$, and $tc$ are the counts of unique unigrams, bigrams, and trigrams in the corpus, $C$ is the corpus, $x \in C$ is a line in the corpus and $w_i \in x$ is a word in a line, and $l(x)$ represents the length of $x$ in $n$-grams.
\begin{equation}
\begin{split}
    p(w_n) &= \frac{\sum_{x \in C} \sum_{n=0}^{l(x)} w_i == w_{n}}{\text{us}} \\
    p(w_{n}, w_{m})  & = \frac{\sum_{x \in C} \sum_{n=0}^{l(x)-1}(w_i == w_{n} \& w_{i+1} == w_m)}{\text{bs}} \\
    p(w_{n}, w_{m}, w_{j})  & = \frac{\sum_{x \in C}\sum_{n=0}^{l(x)-2} (w_i == w_{n} \& w_{i+1} == w_m\& w_{i+2} == w_j) }{\text{ts}} \\
    \text{unigram-} \epsilon({s_i}) &= \prod_{n=0}^{length(s_i)} \log(p(w_n)) \\
       \text{bigram-} \epsilon({s_i}) &= \prod_{n=0}^{length(s_i)-1} \log(p(w_{n-1}, w_n)) \\
       \text{trigram-} \epsilon_{s_i} &= \prod_{n=0}^{length(s_i)-2} \log(p(w_{n}, w_{n+1}, w_{n+2})) 
\end{split}
\label{equation:gramprob}
\end{equation}
\subsubsection{Sentence Dependency Complexity}
There are various methods to define sentence complexity but in our experiments we scope complexity to the complexity of a dependency parse. We leverage the language processing framework spacy \footnote{spacy.io} and for each sentence we generate a dependency parse and starting at the root we measure the depth of the tree. Sentence difficult is formalized in Equation \ref{equation:dep}.
\begin{equation}
    \text{dep-}\epsilon_{s_i} = \text{depth}(s_i)
    \label{equation:dep}
\end{equation}
\subsubsection{Part Of Speech Diversity}
Another core part of language complexity can be derived by the diversity of parts-of-speech in a sentence. We believe that more difficult sentences feature a higher diversity of parts-of-speech (POS). We leverage the part of speech parser from spacy to produce a set of all pos in each sentence. POS Diversity is formalized in Equation \ref{equation:pos}.
\begin{equation}
    \text{pos-}\epsilon_{s_i} = \text{len}(\text{set}(\text{pos}({s_i})))
    \label{equation:pos}
\end{equation}
\subsubsection{Sentence Vs. Line CBC}
As we mentioned prior, our curricula methods are designed for sentence level sampling but most modern LMs train using a context window of line of text or larger. As a result we apply the CBC methods to both the sentence split corpus and a line delimited corpus. For the line delimited corpus we use the existing line breaks in the wikitext-* corpuses and apply our same heuristics at the line level instead of the sentence level. This means we favor short paragraphs far more in our line corpus than our sentence corpus. In our sentence corpus easy sentences will show up earlier in training while in our line corpus if there is a easy sentence which is part of a long line of text, it will not show up until later in training. \\
It is worth noting that since the percentage of padding tokens vary between these two methods line based curricula effectively are larger batches. Since our batch size is limited by our GPU size we instead extend the training time of the sentence based method by roughly 30\%. While there are no guarantees on actual update steps, this approximately allows both methods to train on the same amount of tokens as the original ELMo implementation which is $10 * \text{tokens in corpus}$
\section{Model Training and Evaluation}
\label{chap:method:sec:training}
For model pre-training we follow the original implementation of ELMO and use 2 stacked 4096 dimensional BiLSTMs trained bidirectionally. We use dropout of 0.1, use a batch size of 128, use a context window of 20 tokens, and train for 10 epochs on the full text corpus. In our pre-training we a total of N models. We train 2 baselines (one for each corpus size) using the original ELMo implementation and then train 2 BS models using the same setup. For each of these 4 models they are evaluated on the validation portion of their train corpus at the end of each epoch.  For competence based curricula we train 32 models using our modified implementation: 4 corpuses and 8 curricula per corpus. Since training examples each model sees are different we track model performance over time by evaluating on the validation portion of the train corpus every 100 batches.  Training was done using 3 Nvidia 2080 TI GPUs and training on the wikitext-103 corpus takes about a 30 hours and training on wikitext-2 is under an hour. \\ 
For model fine-tuning everything was implemented using the JIANT framework. Model weights were dumped from the pretrained models and downloaded from the public original ELMo model. Using these weights, each model is fine tuned on each of the GLUE sub-tasks. Using JIANT we normalized our training to have a batch size of 8, random seed of 42 initial learning rate of 0.0001, dropout of 0.2, and a Multi-layer perceptron with 512 hidden dimensions. Training of each model continues until the model learning rate dips below 0.000001 or the model has trained for 1000 epochs. Then, for each sub task, the best result the model predicted is the GLUE score for that task. Each model fine tuning takes about 8 hours using the same training setup. \\
To compare model performance across curricula we will look at 3 different results: model perplexity on held out portion of training corpus, how this perplexity changes over time, and model transfer performance on GLUE. 