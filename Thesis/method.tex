\chapter{Research Approach}
To understand the effects that curriculum learning can have on LM we will explore the effect various curriculum methods have on training, evaluation, and usage in downstream tasks. 
Our methodology is driven around three main research questions:
\begin{enumerate}
\item Does curriculum learning help a Language Model converge to a more optimal global minimum? 
\item Does the language representation learned via curriculum learning improve performance on downstream tasks when compared to non curriculum methods?
\item  Do curriculum learning methods increase model convergence speed in both pre-training and in downstream task fine-tuning?
\end{enumerate}
In this chapter we will first describe the structure of our experiment as scoped to various implementations of curriculum learning. Third, we will describe and discuss our methods for curriculum development. Fourth we will discuss how we will train our Language models and how we will explore hyperparameter tuning. Finally, we  discuss how we evaluate and compare the effect of all of our curriculum's.
\section{Experiment Structure}
Our experimentation strategy is simple: train many models with fixed structure and hyper parameters using different curricula. Our implementation of this strategy is achieved via exploring 2 different curricula: Bengio Style curricula and competence curricula. Once a robust set of models have been trained all models are evaluated on the held out portion of the training corpus and on downstream transfer tasks. What we describe as Bengio style curricula is our implementation of the method originally described by Bengio et al. 09'. For competence based curricula we use a new data loader to sample progressively more complex batches. 
\subsection{Language Model}
To optimize how quickly we can train our systems we will only explore the effect of curriculum on an established successful baseline, ELMO \cite{Smith2019ContextualWR}. We leverage the code used for the original Elmo experiments and keep use the same hyper parameters reported in their paper. We make two changes to the original implementation: an ability to use a competence based curricula and the introduction of a padding token $<PAD>$. In our implementation of competence curricula creates a new data loader. In the original implementation the training loader will load a text file, shuffle all the lines, and then iterate through them. In our implementation, we load the full corpus which we do not shuffle, then select a batch at random from the examples that meet the current competence of our model. This means that our implementation changes data sampling to unconstrained random sampling without replacing to sampling with replacement. Since our implementation of competence curricula is based on sentence level difficulty it becomes possible that batches include sentences that are smaller than the defined context length that meets the original ELMO implementation(20 tokens). As a result we have to introduce a padding token <PAD> which ensures that each sentence is at least the size of the context window. To avoid having the model learn to predict the <PAD> token we introduce a masking function that sets the loss to 0 for any padding token which keeps the model from ever learning to model said token. Each model was trained using 3 Nvidia 2080 Ti GPUs and our code and implementation has been published online \footnote{https://github.com/spacemanidol/ProgressiveLanguageLearning}. 
\subsection{Datasets}
For our training corpus we leverage two well established language modeling benchmarks of wikitext-2 and wikitext-103 \cite{Merity2016PointerSM}. These corpuses are a collection of verified good and featured articles from English Wikipedia and feature 2 million and 103 million tokens. The corpus is composed of full articles with original punctuation, numbers, and case which is not part of the Penn-Tree Bank. Each corpus has already been tokenized and processed and split into train, validation, and evaluation components(80:10:10 based on splitting the source documents). All tokens that occur less than 3 times are replaced with a $<UNK>$ token to represent unknown words. While most other research on Language modeling has been focusing on bigger and bigger data our focus on smaller corpuses allow us to experiment with more methods in curriculum creation. We understand this will limit our model performance relative to current top methods as smaller corpus limits the model performance \cite{Kaplan2020ScalingLF}. Moreover, these corpuses were chosen because they are standard benchmarks for language modeling and of sufficient size to train large scale language models. We believe that the diversity of our two corpuses (103 is 50x larger than 2) will allow us to draw broad conclusion about curriculum learning independent to corpus size. More information about corpus size and diversity can be found in table \ref{table:1}. Information about the corpus size used for other LM is included for comprehensiveness \cite{Chelba2014OneBW}.\\
\begin{table}[h!]
\begin{tabular}{|l|l|l|l|l|} \hline
\textbf{Corpus Name} & \textbf{vocabulary Size} & \textbf{Tokens} & \textbf{lines} & \textbf{sentences} \\ \hline
enwiki-2 & 33278 & 2507007 & 44836 & 131262 \\ \hline
enwiki-103 & 267735 & 103690236 & 1809468  & 5343947 \\ \hline
1B Word Benchmark & 793471 & 829250940 & 0 & 0 \\ \hline
\end{tabular}
Since the context window for ELMO is set to 20 
\caption{Training Corpus details}
\label{table:1}
\end{table}
Its worth noting that as our competence methodology is focused on sentences we make 2 version of the wikitext corpuses: sentence level and line level. For our sentence level corpus we leverage SPACY.IO's sentence boundary detector to spit the original corpus to one delimited by sentences. This mean that each of our competence experiments are run on 4 different corpuses.
\subsection{Evaluation}
To evaluate our models we will focus on three aspects of LM performance: performance on the trained task, performance on a downstream task, and speed which a model improves performances on the trained task. The broad goal of our language models is to accurately represent a given corpus and to do so we will evaluate model perplexity on the held out portions of our datasets. We use the predefined validation portion of the wikitext dataset and evaluate the Bengio style curricula every epoch and the competence curricula every 100 batches. \\
To measure the quality of our Language Representation downstream we will use the industry standard GLUE tasks. GLUE(General Linguistic Understanding Evaluation) \cite{Wang2018GLUEAM} is a collection of 11 natural language evaluation benchmarks ranking for question answering to sentiment detection. This is a standard benchmark for evaluation of language representation and provides a good measure of improvements and magnitude of improvement relative to other systems. Tasks range from question answering, language inference, to paraphrase detection and is broadly considered to be the most representative language understanding benchmark.
\section{Curriculum Construction Methods}
\subsection{Baseline}
Before we study the effect of curriculum learning on language modeling we first retrain the public ELMO model on our datasets and evaluate on GLUE. We also download the publicly released ELMO models and evaluate on GLUE. Our training of ELMO is done on for 10 epochs on the wikitext-2 and wikitext-103.
\subsection{Corpus modification}
Corpus modification follows the implementation covered in the Begio et al. 2009 \cite{Bengio2009CurriculumL} implementation of curriculum learning. In their implementation they trained their model as an Auto regressive model. The curriculum they used is based on vocabulary occurrence. In the first stage of the curriculum the model trains one full epoch(the entire dataset) but all words that are not part of the 5,000 most common are ignored. For the second stage, the corpus is kept constant but only words that are not part of the 10,000 most common are ignored. This process of vocabulary expansion is continued until the model is able to train on the entire vocabulary. In the original implementation the authors see improved model performance and quicker convergence in later stages. \\
In our implementation, we apply this strategy to our two corpuses and generate 7 files per corpus. When a word does not meet the vocabulary threshold are replaced with the $<UNK>$ token. Using these files we train on each for one full epoch and train for an additional 4 epochs on the full vocabulary file. The relative vocabulary size for each of these six splits is corpus dependent with the breakdowns for our dataset found in \ref{table:2}. Note the vocabulary 
\begin{table}[h!]
\begin{tabular}{|l|l|l|l|l|l|l|l|} \hline
\textbf{Corpus Name} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7-*}  \\ \hline
enwiki-2 & 5,000 & 10,000 & 15,000 & 20,000 & 25,000 & 30,000 & 33,278 \\ \hline
enwiki-103 & 25,000 & 50,000 & 75,000 & 100,000 & 150,000  & 250,000 & 267,735\\ \hline
\end{tabular}
\caption{Curriculum Method Vocabulary per epoch}
\label{table:2}
\end{table}
\subsection{Competence Method}
Following the work of \cite{Platanios2019CompetencebasedCL} we will apply the notion of a competence based curriculum to language model training. As covered in depth in our prior work section, competence based methods rely on the ability to assign a difficulty score to each sample in the training corpus and use this to only allow the model to train on problems that are easier than its current competence level. A models competence score is defined by how far along in a training regime the model is. Like the original paper, we set the training length to 90\% the maximum train loss which we find to be $\frac{2}{3}$ of the 10 epochs. At each step in training model competence is represented by $\lambda$ where $\lambda_0 = 0.01$ and $\lambda_t = $.  Prior to training each example in the training data has been assigned a difficulty score from 0 to 1 represented by $\epsilon$. Difficulty scores are assigned by one of the 7 heuristics will shortly describe.  For each training batch, the model is only able to train on samples that have a difficulty we only Then during training a batch can only be made up of samples where $\epsilon <= \lambda$. For wikitext-2 we set the initial competence to 0.1 and the competence increment to 0.0001. For wikitext-103 we set the initial competence to 0.01 and the competence increment to 0.00001.\\
In the rest of this section we denote our
training corpus, $X$ as a collection of sentences $s_1$, where each sentence is a sequence of words $s_i= w_o^i,w_1^i,...,w_n^i$. For the line based corpuses we apply the same difficulty providing heuristic to the entire line.
To produce a difficulty score for each sample we must first define a heuristic which we can use to sort our corpus. To explore what methods produce the greatest improvements in language models we will sort our training data via: sentence length, unigram sentence entropy, bigram sentence entropy, trigram sentence entropy, part of speech diversity, sentence dependency complexity and a random sort. For each methodology, for each $s_i$ in $X$ we compute a difficulty value $\epsilon$. Once the entire corpus is annotated we compute the cumulative density function(CDF) of the difficulty score per sample $s_i\epsilon \in [0,1]$. This method is formalized in Algorithm \ref{algo:competence}. To evaluate the effect of our sampling engine we also implement a baseline curricula. In this curricula the initial competence is set at 1 and thus able to sample randomly from all difficulties of input. \\
\begin{algorithm}[H]
\label{algo:competence}
\SetAlgoLined
\KwResult{Model Trained with Competence Based Curricula}
Compute difficulty, $s_i\epsilon$ for $s_i \in X$\;
Compute Cumulative density function of $s_i\epsilon$\;
Input: Dataset, X = {$s_i$}\;
\For{training step t = 1,...,n}{
Compute model competence $\lambda_t$\;
Sample a data batch $b$ from X such that $s_i\epsilon < \lambda_t$\;
Train on batch $b$
}
\caption{Competence-based curriculum}
\end{algorithm}
\subsubsection{Sentence Length}
We argue that is is a lot harder to model longer sentence, as longer sentences require better tracking of dependencies. We believe this method would be particularly effective in transformer based models as it can steer the model into learning how to leverage its multi-headed attention with different sentence lengths. For each sentence $s_i \in X$ the  $s_i\epsilon = length(s_i)$.
\subsubsection{Sentence Entropy}
Another part of language that can be difficult to model is words with a variety of frequency in the corpora. Models, if assumed to behave like humans, would find it difficult to understand the meaning of a word if they do not see it in a corpus nor have a diversity of usages to infer meaning. Since the statistical strength of training samples with rare words is low and the learning word embeddings are likely to have high variance it is likely that exposing a model early to rare words can result in over and underestimation of true gradients and make convergence difficult. To quantify this difficulty we propose producing a sentence entropy for each sentence with respect to its unigram, bigram, and trigram probabilities. These products can be thought of as an approximate naive language modeling as it assumes words are sampled independently and also implicitly incorporates information as sentence length as the unigram probability decreases and sentence length grows. Note, we are not calculating the conditional probability of each word given the preceding n words but the probability of the n-gram given the text corpus. To produce a difficulty score for each sentence $s_i$ we first calculate a n-gram probability for each unigram equation \ref{equation:unigramprob}, bigram equation \ref{equation:bigramprob}, and trigram equation \ref{equation:trigramprob}in the training corpus. For n-gram difficulty, we calculate the product of all the n-grams in the sentence. For unigrams please see equation \ref{equation:unigramdifficulty}, bigrams please see equation \ref{equation:bigramdifficulty}, and trigrams please see equation \ref{equation:trigramdifficulty}. 
\begin{equation}
    p(w_i) = \sum_{m=0} ^ {length(X)} \sum_{n=0}^{length(s_m)} w_i = w_{nm}
\label{equation:unigramprob}
\end{equation}
\begin{equation}
    p(w_{i}, w_{i+1}) = \sum_{m=0} ^ {length(X)} \sum_{n=0}^{length(s_m)-1} w_i, w_{i+1} = w_{mn},w_{mn+1}
\label{equation:bigramprob}
\end{equation}
\begin{equation}
    p(w_{i}, w_{i+1}, w_{i+2}) = \sum_{m=0} ^ {length(X)} \sum_{n=0}^{length(s_m)-2} w_i, w_{i+1},w_{i+2} = w_{mn},w_{mn+1},w_{mn+2}
\label{equation:trigramprob}
\end{equation}
\begin{equation}
   s_i\epsilon = \prod_{n=0}^{length(s_i)} log(p(, w_n))
\label{equation:unigramdifficulty}
\end{equation}
\begin{equation}
        s_i\epsilon = \prod_{n=0}^{length(s_i)-1} log(p(w_{n-1}, w_n))
\label{equation:bigramdifficulty}
\end{equation}
\begin{equation}
    s_i\epsilon = \prod_{n=0}^{length(s_i)-2} log(p(w_{n}, w_{n+1}, w_{n+2}))
\label{equation:trigramdifficulty}
\end{equation}
\subsubsection{Sentence Complexity}
There are various methods to define sentence complexity but in our experiments we scope complexity to the complexity of a dependency parse. We leverage the language processing framework spacy \footnote{spacy.io} and for each sentence we generate a dependency parse and starting at the root we measure the depth of the tree. This formalizes sentence difficulty for  $s_i \in X$ the  $s_i\epsilon = depth(s_i)$.
\subsubsection{Part Of Speech Diversity}
Another core part of language complexity can be derived by the diversity of parts-of-speech in a sentence. We believe that more difficult sentences feature a higher diversity of parts-of-speech(POS). We leverage the part of speech parser from spacy to produce a set of all pos in each sentence. Using this we formalize sentence difficulty for $s_i \in X$  where $pos_{s_i}$ represents the set of all pos in $s_i$ as $s_i \epsilon = len(pos_{s_i})$.
\section{GLUE Evaluation}
After we have trained all of our ELMO models we will then proceed to evaluate on the GLUE corpus. Our implementation leverages the JIANT system to ensure implementation is consistent across all models. We train 28 different competence curricula models(4 corpuses, 7 curricula), 2 Bengio curricula models , 2 baseline models, and 1 official ELMO model for a total of 33 GLUE systems. Each run uses a batch size of 32, a random seed of 42, initial learning rate of 0.0001, dropout of 0.2, and a Multi layer perceptron with 512 hidden dimensions and is trained for each task until convergence.