\section{Evaluation}
\label{chap:prior:sec:eval}
To understand what the many LMs are doing, the research community has explored many ways to measure language understanding. By no means are language understanding data sets the first use case of natural language benchmarks, but the explosion of NNLM has provided ample testing grounds to explore what various benchmarks are evaluating and its effect. Our scope is narrow and focuses on the evaluation of NNLM regarding language understanding in English. \\
The General Language Understanding Evaluation Benchmark (GLUE) \cite{Wang2018GLUEAM} is a set of resources focused on the evaluation of natural language understanding systems. This benchmark pools nine sentence-level language understanding tasks that seek to cover a diverse range of data type, genre, and difficulty as a proxy for true language understanding. The dataset has been built around a leaderboard format to make benchmarking across language understanding systems more straightforward. Its success has since spawned other benchmarking efforts such as SUPERGLUE \cite{Wang2019SuperGLUEAS}, XTREME \cite{Hu2020XTREMEAM}, and XGLUE \cite{Liang2020XGLUEAN}. \\
The GLUE dataset tasks include question answering, sentiment analysis, and textual entailment, of which the specifics will now be described. It is worth noting that tasks tend to use differing metrics to account for differences in data distributions. The Corpus of Linguistic Acceptability (COLA) \cite{Warstadt2019NeuralNA} consists of 10657 sentences from 23 linguistics publications where each example has been annotated acceptability (grammaticality). It uses Matthews Correlation Coefficient (MCC) as its evaluation metric. The Stanford Sentiment Treebank (SST-2) \cite{Socher2013RecursiveDM} is pairs of movie review sentences and human annotations of their sentiment where the goal of the task is to predict the sentiment of a given sentence. Its evaluation metric is accuracy. The Microsoft Research Paraphrase Corpus (MRPC) \cite{Dolan2005AutomaticallyCA} is a collection of sentence pairs extracted from online news with a human annotation for whether the sentences in the pair are semantically equivalent. MRPC is measured using F1 and Accuracy which are usually separated by a slash or comma. The Quora Question Pairs (QQP) \cite{QQP} consists of question pairs from the question-answering website Quora that have been annotated for semantic equivalence. QQP is measured using F1 and Accuracy which are usually separated by a slash or comma. The Semantic Textual Similarity (STS) Benchmark \cite{Cer2017SemEval2017T1} is a collection of sentences drawn from news data with a human annotation of similarity score (1-5). STS is measured using Pearson Correlation and Spearman Correlation which are usually separated by a slash or comma. The Multi-Genre Natural Language Inference Corpus (MNLI) \cite{Williams2018ABC} is a collection of sentence pairs with textual entailment annotations. MNLI is performance is measured using accuracy. The Stanford Question Answering Dataset (SQUAD) \cite{Rajpurkar2016SQuAD10} is a dataset consisting of question-paragraph pairs, where one of the sentences in the paragraph contains the answer to the corresponding question. In GLUE, this task has been simplified into a binary classification task where the goal is to determine the sentence contains the answer to the question. This modified question answering task is called Question Natural Language Inference (QNLI) and is evaluated in terms of accuracy. The Recognizing Textual Entailment (RTE) dataset is a collection of some of the textual entailment challenges \cite{Dagan2005ThePR} \cite{BarHaim2006TheSP} \cite{Giampiccolo2007TheTP}\cite{Bentivogli2009TheSP} which has been simplified into a binary classification problem. RTE performance is evaluated using accuracy. The Winograd Schema Challenge (WNLI) \cite{Levesque2011TheWS} is a dataset where there are context sentences where ambiguous pronouns have been replaced with each possible referent, and there is a binary classification task if this is the correct referent or not. WNLI performance is evaluated using accuracy. Finally, the Diagnostic Dataset (DX) is a handpicked set of examples from the MNLI corpus used to evaluate individual linguistic phenomena and it uses MCC as its evaluation metric. Using the average of performance on each task a GLUE score is created which provides an overall sorting metric to compare LM performance. \\ 
To ensure that NLP model evaluation (of which GLUE is an example) is done in a consistent and reproducible method, the JIANT toolkit was developed \cite{Pruksachatkun2020jiantAS}. JIANT is an open-source tool for conducting multi-task and transfer learning experiments in English to implement the GLUE benchmark. JIANT builds on the notion of a configuration which provides all settings needed to run and reproduce an experiment in a simple text file. JIANT provides consistent data processing, classifier implementation, and evaluation to ensure that users of the framework can focus on the outputs and not worry about implementing benchmarking tasks like GLUE.