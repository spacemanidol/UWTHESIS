\section{Evaluation}
\label{chap:prior:sec:eval}
To understand what the many LMs are doing, the research community has explored many ways to measure language understanding. By no means are language understanding data sets the first use case of natural language benchmarks, but the explosion of NNLM has provided ample testing grounds to explore what various benchmarks are evaluating and its effect. Our scope is narrow and focuses on the evaluation of NNLM regarding language understanding in English. \\
The General Language Understanding Evaluation Benchmark (GLUE) \cite{Wang2018GLUEAM} is a set of resources focused on the evaluation of natural language understanding systems. This benchmark pools nine sentence-level language understanding tasks that seek to cover a diverse range of data type, genre, and difficulty as a proxy for true language understanding. The dataset has been built around a leaderboard format to make benchmarking across language understanding systems more straightforward. Its success has since spawned other benchmarking efforts such as SUPERGLUE \cite{Wang2019SuperGLUEAS}, XTREME \cite{Hu2020XTREMEAM}, and XGLUE \cite{Liang2020XGLUEAN}. \\
The GLUE dataset tasks include question answering, sentiment analysis, and textual entailment, of which the specifics will now be described. The Corpus of Linguistic Acceptability (COLA) \cite{Warstadt2019NeuralNA} consists of 10657 sentences from 23 linguistics publications where each example has been annotated acceptability (grammaticality). The Stanford Sentiment Treebank (SST-2) \cite{Socher2013RecursiveDM} is pairs of movie review sentences and human annotations of their sentiment where the goal of the task is to predict the sentiment of a given
sentence. The Microsoft Research Paraphrase Corpus (MRPC) \cite{Dolan2005AutomaticallyCA} is a collection of sentence pairs extracted from online news with a human annotation for whether
the sentences in the pair are semantically equivalent. The Quora Question Pairs (QQP) \cite{QQP} consists of question pairs from the question-answering website Quora that have been annotated for semantic equivalence. The Semantic Textual Similarity (STS) Benchmark \cite{Cer2017SemEval2017T1} is a collection of sentences drawn from news data with a human annotation of similarity score (1-5). The Multi-Genre Natural Language Inference Corpus (MNLI) \cite{Williams2018ABC} is a collection of sentence pairs with textual entailment annotations. The Stanford Question Answering Dataset (SQUAD) \cite{Rajpurkar2016SQuAD10} is a dataset consisting of question-paragraph pairs, where one of the sentences in the paragraph contains the answer to the corresponding question. In GLUE, this task has been simplified into a binary classification task where the goal is to determine the sentence contains the answer to the question. This modified question answering task is called QNLI. The Recognizing Textual Entailment (RTE) dataset is a collection of some of the textual entailment challenges \cite{Dagan2005ThePR} \cite{BarHaim2006TheSP} \cite{Giampiccolo2007TheTP}\cite{Bentivogli2009TheSP} which has been simplified into a binary classification problem. The Winograd Schema Challenge (WNLI) \cite{Levesque2011TheWS} is a dataset where there are context sentences where ambiguous pronouns have been replaced with each possible referent, and there is a binary classification task if this is the correct referent or not. Together these datasets provide a global score that serves as a point of comparison across models. GLUE presents a broad benchmark designed to evaluate a model's understanding of language concerning a complex set of goals. \\ 
To ensure that NLP model evaluation (of which GLUE is an example) is done in a consistent and reproducible method, the JIANT toolkit was developed \cite{Pruksachatkun2020jiantAS}. JIANT is an open-source tool for conducting multi-task and transfer learning experiments in English to implement the GLUE benchmark. JIANT builds on the notion of a configuration which provides all settings needed to run and reproduce an experiment in a simple text file. JIANT provides consistent data processing, classifier implementation, and evaluation to ensure that users of the framework can focus on the outputs and not worry about implementing benchmarking tasks like GLUE.